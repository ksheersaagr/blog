<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="https://ksheersaagr.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ksheersaagr.github.io/blog/" rel="alternate" type="text/html" /><updated>2024-08-19T07:02:21+00:00</updated><id>https://ksheersaagr.github.io/blog/feed.xml</id><title type="html">Krunal Kshirsagar</title><entry><title type="html">[Thoughts]-On the interpretability conundrum</title><link href="https://ksheersaagr.github.io/blog/2022/01/on-interpretability-conundrum/" rel="alternate" type="text/html" title="[Thoughts]-On the interpretability conundrum" /><published>2022-01-05T00:00:00+00:00</published><updated>2022-01-05T00:00:00+00:00</updated><id>https://ksheersaagr.github.io/blog/2022/01/on-interpretability-conundrum</id><content type="html" xml:base="https://ksheersaagr.github.io/blog/2022/01/on-interpretability-conundrum/"><![CDATA[<p>Of all the unknowable information, you can only get as much unknowable information out of 
a  system  as  axioms  you  put  in.  Better  understanding depends on  the  amount  of  input 
axioms. Better prediction requires better understanding. And better understanding makes a 
system  more  interpretable.  Higher  the  interpretability  of a system  the  higher  we  trust the 
system. Interpretability focuses on understanding the cause of the decision while inference 
focuses on the conclusion reached on the basis of evidence &amp; reasoning(Causal &amp; Bayesian).</p>

<p>However there’s a general notion of trade-off between the accuracy and interpretability of the 
system let alone the privacy aspect. The most affected domain by this trade-off is the medical domain. The 
problem is  that the  systems  are heavily biased; be it racial, gender or any other biases that 
you can think of. The ML models can predict self-reported race even from corrupted, cropped, and 
noised medical images as opposed to medical experts. 
These  ML  models  are  making accurate decisions  about  racial  classification using features that humans can’t even notice carefully, let alone analyse. <a class="citation" href="#readingracebanerjee2021">(Banerjee et al., 2021)</a>
<br />
<img src="/blog/images/2022-01-01-trilemma-or-trilogy.png" style="float: right; max-width: 50%; margin: 0 0 1em 2em;" /></p>

<p>In order to trust the system 
we need to break things down like in the first principles approach and then make a ground-up 
approach  to  interpretability/reasoning.  Hence,  I  believe,  mathematical  methods  like  causal 
inference, differential &amp; algebraic geometry, topology, stability theory, probabilistic methods, 
PDEs, information geometry &amp;  algorithmic information theory can help  achieve better interpretability. I 
believe with mathematical proofs and truths we can achieve inductive/abductive reasoning and 
inference  while  discarding  the  role  of  medical  domain  experts  because  humans  have  a 
tendency to lie, mathematics doesn’t lie. Math was already there,  humans just discovered it 
they  didn’t  invent  it.  Math  should  be  the  base  of  the  prediction,  inference  and  reasoning 
instead of a ‘Domain level expert human being’. Thus, in my opinion the model should learn 
from the mathematical proofs instead (learn from nature, don’t learn from humans).</p>

<!---

False positives, connectomes

Roses aren't red,<br>
The sky isn't blue,<br>
It's your perception you idiot,<br>
that's messing up with you.<br>

**Even Shane Warne knows why interpretability is important :p**

<blockquote class="twitter-tweet" tw-align-center><p lang="en" dir="ltr">This is simply - not out !!!!! We often discuss technology &amp; its use / accuracy. The main problem@is the interpretation of the technology. Here’s a perfect example of the ball clearly hitting the edge of the bat first. <a href="https://t.co/OATRzIHcfg">https://t.co/OATRzIHcfg</a></p>&mdash; Shane Warne (@ShaneWarne) <a href="https://twitter.com/ShaneWarne/status/1466958968735952897?ref_src=twsrc%5Etfw">December 4, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



## Trilemma or Trilogy?
<img src="/blog/images/2022-01-01-trilemma-or-trilogy.png"
style="float: right; max-width: 50%; margin: 0 0 1em 2em;"> 


Limitation of knowledge-say I don't know if you really don't understand it instead of assuming and confidently lying (false positives)

## Reasoning

Connecting the dots thats reasoning instead of processing the whole information all at once and arriving to a conclusion.

- Rigorous proof:

    Set theory, logic and geometry.

### Inductive vs Deductive approach:

 

## Science v/s Math

'Mathematics is a language plus reasoning; it is like a language plus logic. Mathematics is tool for reasoning'. ~Richard Feynman

### Trusting the human v/s trusting the nature (math)

**_Don't listen to the 'expert'!_**
The press secretary ~Robin Hanson (Economist and Advisor at the Future of humanity institute of the oxford university)-humans lie to support their narrative by sacrificing the truth. (Malcolm Gladwell fallacy)

Mathematical proofs and truths over governance and law?!

different experts will have different interpretation.
Humans aren't absolute, Math is absolute.

Example:
<blockquote class="twitter-tweet" tw-align-center><p lang="en" dir="ltr">It’s a clear indication of find the pictures to suit your narrative is all that is. In the side view the bat has not reached the ball by the time the ball reaches the pad so there for its safe to say hitting the pad first as its directly in the same line did happen first. <a href="https://twitter.com/hashtag/simple?src=hash&amp;ref_src=twsrc%5Etfw">#simple</a></p>&mdash; Simon Doull (@Sdoull) <a href="https://twitter.com/Sdoull/status/1467007579263934468?ref_src=twsrc%5Etfw">December 4, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

--->

<p><strong><code class="highlighter-rouge">'If I bet on humanity, I'd never cash a ticket. It didn't pay to trust another human being. They never had it from the beginning, whatever it took'</code>.</strong> <a class="citation" href="#women1978">(Bukowski, 1978)</a></p>

<!---
## References:

[^1]: Banerjee, I., Bhimireddy, A. R., Burns, J. L., Celi, L. A., Chen, L.-C., Correa, R., Dullerud, N., Ghassemi, M., Huang, S.-C., Kuo, P.-C., Lungren, M. P., Palmer, L. J., Price, B. J., Purkayastha, S., Pyrros, A., Oakden-Rayner, L., Okechukwu, C., Seyyed-Kalantari, L., Trivedi, H., Wang, R., Zaiman, Z., Zhang, H., Gichoya, J. W. (2021). Reading Race: AI Recognises Patient’s Racial Identity In Medical Images. CoRR, abs/2107.10356. https://arxiv.org/abs/2107.10356

[^2]: Bukowski, C. (1978). In *Women*. HarperCollins.

--->]]></content><author><name>krunal kshirsagar</name></author><summary type="html"><![CDATA[Of all the unknowable information, you can only get as much unknowable information out of a system as axioms you put in. Better understanding depends on the amount of input axioms. Better prediction requires better understanding. And better understanding makes a system more interpretable. Higher the interpretability of a system the higher we trust the system. Interpretability focuses on understanding the cause of the decision while inference focuses on the conclusion reached on the basis of evidence &amp; reasoning(Causal &amp; Bayesian).]]></summary></entry><entry><title type="html">Switching to Pop! OS for machine learning made easy</title><link href="https://ksheersaagr.github.io/blog/2021/12/Switching-to-Pop-OS-for-machine-learning-made-easy/" rel="alternate" type="text/html" title="Switching to Pop! OS for machine learning made easy" /><published>2021-12-04T00:00:00+00:00</published><updated>2021-12-04T00:00:00+00:00</updated><id>https://ksheersaagr.github.io/blog/2021/12/Switching-to-Pop-OS-for-machine-learning-made-easy</id><content type="html" xml:base="https://ksheersaagr.github.io/blog/2021/12/Switching-to-Pop-OS-for-machine-learning-made-easy/"><![CDATA[<p>Some of the major components of Pop!_OS are written in Rust. Its 21.04 version was released on 29 June 2021 and is based upon Ubuntu 21.04. It included the COSMIC (Computer Operating System Main Interface Components) desktop, based on GNOME, but with a custom dock and shortcut controls. Also, it is said that in the future updates System76 will be ditching GNOME and will introduce a new Desktop Environment all together written in Rust (expect it to be heck of alot faster!) based on Pop! OS instead. Hmm..Going the apple way it seems but remaining open source of course. However it won’t be an easy task, wish them luck and I hope it comes out.</p>

<p>References: <a href="https://www.reddit.com/r/linux_gaming/comments/qo4bdo/pop_os_to_use_it_own_desktop_environment_written/">[here]</a><a href="https://www.reddit.com/r/linux/comments/qododc/pop_os_to_build_a_new_independent_desktop/">[here]</a></p>

<h1 id="table-of-contents">Table of Contents</h1>

<ol>
  <li><a href="#setup">Setup</a>
    <ul>
      <li><a href="#download">Download</a></li>
      <li><a href="#bootable-drive">Making a bootable USB drive</a></li>
      <li><a href="#installing-os">Installing Pop! OS</a></li>
      <li><a href="#update-upgrade">Initial Update &amp; Upgrade</a></li>
      <li><a href="#maximise-button">Enable maximise button</a></li>
      <li><a href="#dash-dock">Dash to Dock</a></li>
      <li><a href="#audio">Audio</a>
        <ul>
          <li><a href="#restricted-formats">Installing Restricted Formats</a></li>
          <li><a href="#sound-issues">Sound Issues</a></li>
        </ul>
      </li>
      <li><a href="#backup-restore">Backup &amp; Restore(Not Mandatory)</a></li>
    </ul>
  </li>
  <li><a href="#installing-conda">Installing Conda</a>
    <ul>
      <li><a href="#conda-commands">Useful conda commands</a></li>
    </ul>
  </li>
  <li><a href="#julia-lang">Installing Julia</a>
    <ul>
      <li><a href="#ijulia-kernel">Installing IJulia kernel</a></li>
      <li><a href="#cuda-jl">Installing CUDA.jl</a></li>
    </ul>
  </li>
  <li><a href="#installing-nodejs">Installing nodejs(Optional)</a></li>
  <li><a href="#installing-git">Installing Git</a></li>
  <li><a href="#installing-pytorch">Installing Pytorch</a>
    <ul>
      <li><a href="#pytorch-geometric">Installing Pytorch Geometric</a></li>
    </ul>
  </li>
  <li><a href="#installing-cuda">Installing Cuda</a>
    <ul>
      <li><a href="#installing-cuda-toolkit">Installing Cuda toolkit</a></li>
      <li><a href="#installing-cudnn">Installing CuDNN</a></li>
    </ul>
  </li>
  <li><a href="#installing-jax">Installing JAX</a></li>
  <li><a href="#installing-vs-code">Installing VS Code</a></li>
</ol>

<h2 id="1-setup">1. <a name="setup">Setup</a></h2>

<h3 id="-download-"><a name="download"> Download </a></h3>
<ul>
  <li>Download Pop! OS from <strong><a href="https://pop.system76.com/">here</a></strong>. click <strong>Download</strong>, then choose from the current release (default) or if you have Nvidia GPU then please download the <strong>NVIDIA</strong> version(Pop! OS provide Nvidia drivers for GPU support out of the box).</li>
  <li>Download PowerISO from <strong><a href="https://www.poweriso.com/download.php">here</a></strong>.</li>
</ul>

<h3 id="making-a-bootable-usb-drive-"><a name="bootable-drive">Making a bootable USB drive </a></h3>
<p>Install the poweriso.exe file. After installing, open the poweriso software then go to <strong>Tools</strong> =&gt; <strong>Create bootable USB drive</strong> then hit OK, then <strong>select the source file(Pop! OS image file with .iso extension)</strong>, <strong>select destination USB drive</strong> and hit <strong>Start</strong>. It will take couple of minutes to complete the task.</p>

<h3 id="installing-pop-os"><a name="installing-os">Installing Pop! OS</a></h3>
<ol>
  <li>Insert the bootable usb into the usb slot and reboot the system.</li>
  <li>Keep hitting <strong>F10 or F11</strong> key depending on your systems motherboard (mine is of MSI - so it’s F11 key) while your system restarts.</li>
  <li>In the bootloader menu select your bootable usb drive.</li>
  <li>Next select the options of your choice - mainly the language, time zone &amp; keyboard layout etc.(better keep it to default)</li>
  <li>Once you are in the installation window you’ll see 2 options:
    <ol>
      <li>Clean Install.(<strong>Warning: This will wipe all of your system’s data</strong>)</li>
      <li>Custom (Advanced).
 choose as per your need, I chose the 1st option to clear everything and install a fresh copy of Pop! OS.</li>
    </ol>
  </li>
  <li>Then select the disk drive that you want to install the Pop! OS a fresh and hit <strong>Erase and Install.</strong></li>
  <li>Drive Encryption: select if you want to encrypt your drive. (This will impact the performance of your system) - I chose don’t encrypt.</li>
  <li>And then the usual stuff like select the wifi, time zone, keyboard layout, privacy location settings and connect account etc - choose as you need to and you are done with the installation.</li>
</ol>

<h3 id="initial-update--upgrade"><a name="update-upgrade">Initial Update &amp; Upgrade</a></h3>
<p>In order to upgrade your system, execute the following commands in the terminal:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    sudo apt update
    sudo apt full-upgrade
</code></pre></div></div>

<h3 id="enable-maximise-button"><a name="maximise-button">Enable maximise button</a></h3>
<p>Since it’s a highly customisable OS, some functionality might not be available out of the box. For instance, the maximise button isn’t available for you to maximise the window size instead you have to double click on top of the window pane in order to maximise the window size.</p>

<p>Let’s add the maximise button with the following command:</p>

<p><code class="highlighter-rouge">sudo apt install gnome-tweaks</code></p>

<p>Now go to applications and select <strong>tweaks</strong>. When it opens click on the <strong>Window Titlebars</strong> tab to the left and then enable <strong>Maximise</strong> toggle option under the <strong>Titlebar Buttons</strong> section.</p>

<p>Check out all the different settings and configuration options to get the most out of your system.</p>

<h3 id="dash-to-dock"><a name="dash-dock">Dash to Dock</a></h3>
<p>There are a few gripes that I have but nothing that can’t be resolved with some more time under my belt with the new OS. My biggest and probably the most worrisome is the <strong>taskbar</strong>. I don’t really like that I can’t see all of my running apps on the bar. It makes switching a bit of a pain, although alt+tab isn’t the worst in the world, I would like another option if I’m having a derp moment. Please find how to customise the taskbar <strong><a href="https://support.system76.com/articles/dash-to-dock/">here</a>.</strong></p>

<p><strong>Note: once you are done with the installation you should find Dash-to-Dock settings by right-clicking on the <code class="highlighter-rouge">show application</code> menu.</strong></p>

<h3 id="audio"><a name="audio">Audio</a></h3>
<h4 id="installing-restricted-formats"><a name="restricted-formats">Installing Restricted Formats</a></h4>
<p>Pop! OS comes with few of the open source softwares. However, you will find some codecs or media format missing. Execute following commands in the terminal:</p>

<p><code class="highlighter-rouge">sudo apt-get install ubuntu-restricted-extras</code></p>

<p>Oh! and in the package configuration window use <strong>Tab</strong> button to select OK and Yes options.</p>

<h4 id="sound-issues"><a name="sound-issues">Sound Issues</a></h4>
<p>If you come across a sound issue, please run the following commands in the terminal and reboot the system:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    1. sudo apt purge timidity-daemon
    2. sudo apt-get install --reinstall alsa-base pulseaudio
    3. sudo alsa force-reload
</code></pre></div></div>

<h3 id="backup--restore"><a name="backup-restore">Backup &amp; Restore</a></h3>
<p>Install Timeshift by running the following command in the terminal:</p>

<p><code class="highlighter-rouge">sudo apt install timeshift</code></p>

<p>After installing follow the instructions in the video below:</p>

<div class="video">
<div class="embed-container">
    <iframe src="https://www.youtube.com/embed/QE0lyWodWdU" width="700" height="480" frameborder="0" allowfullscreen="">
    </iframe>
  </div>
<div style="text-align:center">Timeshift tutorial.</div>
</div>

<p>and you are done with the initial setup.</p>

<h2 id="2-installing-conda">2. <a name="installing-conda">Installing Conda</a></h2>
<ol>
  <li>
    <p>Download the latest <code class="highlighter-rouge">.sh</code> file for linux from <strong><a href="https://www.anaconda.com/products/individual">here</a></strong>.</p>
  </li>
  <li>
    <p>Open a new terminal and go to your Downloads folder:</p>

    <p><code class="highlighter-rouge">cd ~/Downloads</code></p>
  </li>
  <li>
    <p>Run the following command in the terminal:</p>

    <p><code class="highlighter-rouge">bash ~/Downloads/Anaconda3-2021.11-Linux-x86_64.sh</code></p>

    <p><strong>Note: please change the command as per the <code class="highlighter-rouge">.sh</code> file name.</strong></p>
  </li>
  <li>Now keep pressing <strong>Enter</strong> till it asks -
    <ol>
      <li>Do you accept the license terms? -&gt; Type <strong>yes</strong> and hit <strong>Enter</strong>.</li>
      <li>Do you wish the installer to initialize Anaconda3 by running conda init? -&gt; Type yes and hit <strong>Enter</strong>.</li>
    </ol>
  </li>
  <li>
    <p>Then re-open the terminal and enter the following command:</p>

    <p><code class="highlighter-rouge">source ~/.bashrc</code></p>
  </li>
  <li>
    <p>Again open a new terminal and enter:</p>

    <p><code class="highlighter-rouge">conda -V</code></p>
  </li>
</ol>

<p>if it returns the conda version then the installation is successful.</p>

<h3 id="useful-conda-commands"><a name="conda-commands">Useful conda commands</a></h3>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    1. conda activate
    2. conda init
    3. jupyter notebook 
</code></pre></div></div>

<h2 id="3-installing-julia">3. <a name="julia-lang">Installing Julia</a></h2>
<ol>
  <li>
    <p>Download the <code class="highlighter-rouge">tar.gz</code> file from <strong><a href="https://julialang.org/downloads/">here</a></strong>.
<strong>Under the <code class="highlighter-rouge">Current stable release:</code> section, select the <code class="highlighter-rouge">64-bit</code> version from <code class="highlighter-rouge">Generic Linux on x86</code></strong>.</p>
  </li>
  <li>
    <p>Open a new terminal and go to your Downloads folder:</p>

    <p><code class="highlighter-rouge">cd ~/Downloads</code></p>
  </li>
  <li>
    <p>Extract the <code class="highlighter-rouge">.tar.gz</code>:</p>

    <p><code class="highlighter-rouge">tar -xvzf julia-1.6.4-linux-x86_64.tar.gz</code></p>
  </li>
  <li>
    <p>Copy the extracted folder to <code class="highlighter-rouge">/opt</code>:</p>

    <p><code class="highlighter-rouge">sudo cp -r julia-1.6.4 /opt/</code></p>
  </li>
  <li>
    <p>Create a symbolic link to julia inside the /usr/local/bin folder:</p>

    <p><code class="highlighter-rouge">sudo ln -s /opt/julia-1.6.4/bin/julia /usr/local/bin/julia</code></p>
  </li>
  <li>
    <p>Re-open the terminal and execute the command:</p>

    <p><code class="highlighter-rouge">julia</code></p>
  </li>
</ol>

<h3 id="installing-ijulia-kernel"><a name="ijulia-kernel">Installing IJulia kernel</a></h3>
<p>In order to run Julia in Jupyter notebook, you need to install the IJulia kernel by entering the following command in the Julia REPL terminal:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    1. using Pkg
    2. Pkg.add("IJulia")
</code></pre></div></div>
<p>Once installed, re-open the terminal and open jupyter notebook by entering <code class="highlighter-rouge">jupyter notebook</code> command, now in the browser, click <strong>new</strong> option and you’ll see the Julia kernel mentioned in the dropdown menu, hit the Julia kernel option and you will be redirected to a new notebook with IJulia kernel. Now you can write Julia programs in Jupyter notebooks.</p>

<h3 id="installing-cudajl"><a name="cuda-jl">Installing CUDA.jl</a></h3>
<p>CUDA.jl package is the main programming interface for working with NVIDIA CUDA GPUs using Julia. Install the package by entering the following command in the Julia REPL terminal:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    1. using Pkg
    2. Pkg.add("CUDA")
</code></pre></div></div>

<h2 id="4-installing-nodejs">4. <a name="installing-nodejs">Installing nodejs</a></h2>
<p>Install nodejs by executing following commands in the terminal:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    1. sudo apt update
    2. sudo apt install nodejs npm
</code></pre></div></div>
<p>To check whether nodejs is properly installed or not, launch REPL terrminal by pressing <strong><code class="highlighter-rouge">Ctrl+C</code> twice</strong> &amp; execute the commands given below:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    1. nodejs
    2. node -V &amp;&amp; npm -V
</code></pre></div></div>
<p>If it returns version then nodejs is properly installed.</p>

<h2 id="5-installing-git">5. <a name="installing-git">Installing Git</a></h2>
<p>Run the following commands in the terminal and you are good to go:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    1. sudo apt-get install git-all
    2. git --version
</code></pre></div></div>

<h2 id="6-installing-pytorch">6. <a name="installing-pytorch">Installing Pytorch</a></h2>
<p>Head out to <strong><a href="https://pytorch.org">pytorch.org</a></strong> and you’ll see a command customised according to your environment under the <strong>Run this Command</strong> section which should look like this:</p>

<p><strong><code class="highlighter-rouge">conda install pytorch torchvision cudatoolkit=11.3 -c pytorch</code></strong></p>

<p>or you can customised it according to your needs.</p>

<h3 id="installing-pytorch-geometric"><a name="pytorch-geometric">Installing Pytorch Geometric</a></h3>
<p>Run the following command to install:</p>

<p><strong><code class="highlighter-rouge">conda install pyg -c pyg -c conda-forge</code></strong></p>

<h2 id="7-installing-cuda">7. <a name="installing-cuda">Installing Cuda</a></h2>

<!--
Install the Cuda toolkit by entering the following command in the terminal:

**`sudo apt install nvidia-cuda-toolkit`**

after installing check if it's installed properly by entering the following command in the terminal:

```
1. nvidia-smi
2. nvcc --version
```
if it returns the version then it's properly installed.

- check if the pytorch recognises the cuda is installed or not and is available by running the following command in the terminal:

    ```
    1. python
    2. import pytorch 
    3. torch.cuda.is_available()
    ```
if it returns **`True`** then it's installed properly.
-->
<h3 id="installing-cuda-toolkit"><a name="installing-cuda-toolkit">Installing Cuda toolkit</a></h3>

<ol>
  <li>
    <p>Click <strong><a href="https://developer.nvidia.com/cuda-downloads">here</a></strong> to redirect to the official CUDA Toolkit website.</p>
  </li>
  <li>
    <p>Now under <strong>Select Target Platform</strong>, choose <strong>Linux</strong> then select <strong>x86_64</strong> as Architecture, then select <strong>Ubuntu</strong> as Distribution, select <strong>20.04</strong> as Version, lastly select <strong>runfile[local]</strong> as Installer type.</p>
  </li>
  <li>
    <p>Scroll to the <strong>Base Installler</strong> section and under <strong>Installation Instructions</strong> copy the 1st command that will look like this:</p>

    <p><strong><code class="highlighter-rouge">wget https://developer.download.nvidia.com/compute/cuda/11.5.1/local_installers/cuda_11.5.1_495.29.05_linux.run</code></strong></p>

    <p>Now paste &amp; run the above command in the terminal.</p>
  </li>
  <li>
    <p>Now run the <strong><code class="highlighter-rouge">ls</code></strong> command in the terminal, you will see <strong><code class="highlighter-rouge">cuda_11.5.1_495.29.05_linux.run</code></strong> mentioned in the output, copy the name of the file. After that run the following command in the terminal to give execute permission:</p>

    <p><strong><code class="highlighter-rouge">sudo chmod +x cuda_11.5.1_495.29.05_linux.run</code></strong></p>
  </li>
  <li>
    <p>Then copy the 2nd command from the <strong>Installation Instructions</strong>, paste &amp; run it in the terminal:</p>

    <p><strong><code class="highlighter-rouge">sudo sh cuda_11.5.1_495.29.05_linux.run</code></strong></p>

    <p>After that select <strong>Continue</strong> and hit <strong>Enter</strong>. Then write <strong>accept</strong> and hit <strong>Enter</strong> again.</p>

    <p>Next you’ll be in the CUDA Installer window, in that select <strong>CUDA Toolkit 11.5</strong> and deselect every other options and select <strong>Install</strong>. You’ll see the summary after successful installation <strong>(It will be mentioned that <code class="highlighter-rouge">Toolkit:  Installed in /usr/local/cuda-11.5/</code>)</strong>.</p>
  </li>
  <li>
    <p>Open the .bashrc by typing the following command in the terminal:</p>

    <p><strong><code class="highlighter-rouge">nano ~/.bashrc</code></strong></p>

    <ul>
      <li>Set the PATH by copying the following line to the file:</li>
    </ul>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> export PATH=/usr/local/cuda-11.5/bin${PATH:+:${PATH}}

 export LD_LIBRARY_PATH=/usr/local/cuda-11.5/lib64\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}

 export CUDADIR=/usr/local/cuda-11.5
</code></pre></div>    </div>
    <p>use <strong><code class="highlighter-rouge">Ctrl+Alt+S</code></strong> to save, then use <strong><code class="highlighter-rouge">Ctrl+X</code></strong> to Exit, then it will ask to save modified buffer? - type <strong><code class="highlighter-rouge">Y</code> for yes</strong> and then it’ll ask to save the file hit <strong><code class="highlighter-rouge">Enter</code></strong>.</p>

    <p>After that paste following command in the terminal to force load and re-read the .bashrc file:</p>

    <p><strong><code class="highlighter-rouge">source ~/.bashrc</code></strong></p>

    <p>You are done with CUDA Toolkit Installation.</p>
  </li>
  <li>
    <p>Some useful stuff about Cuda toolkit:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> 1. https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html

 2. https://stackoverflow.com/questions/53422407/different-cuda-versions-shown-by-nvcc-and-nvidia-smi

 3. https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html

 4. https://askubuntu.com/questions/1211919/error-installing-cuda-toolkit-existing-package-manager-installation-of-the-driv

</code></pre></div>    </div>
  </li>
</ol>

<h3 id="installing-cudnn"><a name="installing-cudnn">Installing CuDNN</a></h3>

<ol>
  <li>
    <p>Click <strong><a href="https://developer.nvidia.com/rdp/cudnn-archive">here</a></strong> to redirect to the official cuDNN archive website.</p>
  </li>
  <li>
    <p>Select the latest cuDNN installation. In my case I chose the <strong>cuDNN v8.3.0(November 3rd, 2021), for CUDA 11.5</strong>
It will ask you to sign up to Nvidia developers program, sign up using any dummy email address then click the link in your email address to verify the sign up and then submit the form. After that the file will be downloaded.</p>
  </li>
  <li>
    <p>Navigate to where you have downloaded the cudnn tar file. In my case I’ve downloaded it to the Downlods directory. So change directory to Downloads by <strong><code class="highlighter-rouge">cd Downloads/</code></strong> after that unzip the CuDNN package .tgz file using the following command in the terminal:</p>

    <p><strong><code class="highlighter-rouge">tar -xvf cudnn-11.5-linux-x64-v8.3.0.98.tgz</code></strong></p>
  </li>
  <li>
    <p>Copy the following files into the CUDA toolkit directory by entering the following command in the terminal:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> 1. sudo cp cuda/include/cudnn*.h /usr/local/cuda/include

 2. sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda/lib64

 3. sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*
</code></pre></div>    </div>
  </li>
  <li>
    <p>Reboot the system and enter the following command in the terminal:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> 1. nvidia-smi

 2. nvcc --version
</code></pre></div>    </div>
  </li>
  <li>
    <p>Some useful stuffs about CudNN:</p>
    <ul>
      <li>
        <p>Check CudNN by running the following command:</p>

        <p><code class="highlighter-rouge">cat ${CUDNN_H_PATH} | grep CUDNN_MAJOR -A 2</code></p>

        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              OR
</code></pre></div>        </div>

        <p><code class="highlighter-rouge">sudo cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A2</code></p>

        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              OR
</code></pre></div>        </div>

        <p><code class="highlighter-rouge">CUDNN_H_PATH=$(whereis cudnn.h)</code></p>
      </li>
      <li>
        <p>More References:</p>

        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
  1. https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#installlinux

  2. https://support.system76.com/articles/cuda/

  3. https://newbedev.com/how-to-verify-cudnn-installation

</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ol>

<h2 id="8-installing-jax">8. <a name="installing-jax">Installing JAX</a></h2>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. pip install --upgrade pip

2. pip install jax[cuda11_cudnn82] -f https://storage.googleapis.com/jax-releases/jax_releases.html

</code></pre></div></div>

<p><strong>Warning: This will install jax compatible with latest version of CUDA &amp; CuDNN. As I’m writing this post, the latest JAX version is compatible with CUDA 11 &amp; CuDNN 8.2 or newer respectively.</strong></p>

<ul>
  <li>
    <p>Check if JAX is installed properly with GPU support by running the following commands in the terminal:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  1. python

  2. from jax.lib import xla_bridge

  3. print(xla_bridge.get_backend().platform)

</code></pre></div>    </div>

    <p><strong>If installed properly with GPU support, it will print <code class="highlighter-rouge">GPU</code> in the terminal.</strong></p>
  </li>
  <li>
    <p>Reference:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    
  https://github.com/google/jax#installation
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="9-installing-vs-code">9. <a name="installing-vs-code">Installing VS Code</a></h2>

<ol>
  <li>
    <p>Go to Pop! shop from the application menu and search for the VS Code</p>
  </li>
  <li>
    <p>Select VS Code and hit install.</p>
  </li>
</ol>

<p>Note that in VS Code, search for any extensions you require by pressing <strong><code class="highlighter-rouge">Ctrl+Shift+X</code></strong>.</p>

<p><strong>Note: New stuff will be added to this blog post if I discover some useful things related to Pop! OS for machine learning.</strong></p>

<!-- 
<div class="imgcap">
<img src="/blog/images/2019-12-01-facial-keypoint-detection-using-cnn-pytorch-dots-represents-keypoints.png">
<div class="thecap">Examples of RL in the wild. <b>From left to right</b>: Deep Q Learning network playing ATARI, AlphaGo, Berkeley robot stacking Legos, physically-simulated quadruped leaping over terrain.</div>
</div>
-->]]></content><author><name>krunal kshirsagar</name></author><summary type="html"><![CDATA[Some of the major components of Pop!_OS are written in Rust. Its 21.04 version was released on 29 June 2021 and is based upon Ubuntu 21.04. It included the COSMIC (Computer Operating System Main Interface Components) desktop, based on GNOME, but with a custom dock and shortcut controls. Also, it is said that in the future updates System76 will be ditching GNOME and will introduce a new Desktop Environment all together written in Rust (expect it to be heck of alot faster!) based on Pop! OS instead. Hmm..Going the apple way it seems but remaining open source of course. However it won’t be an easy task, wish them luck and I hope it comes out.]]></summary></entry><entry><title type="html">Suckerbug: A python script to scrape photos from facebook’s public pages</title><link href="https://ksheersaagr.github.io/blog/2021/10/suckerbug-python-script-to-scrape-photos-from-facebook-public-pages/" rel="alternate" type="text/html" title="Suckerbug: A python script to scrape photos from facebook’s public pages" /><published>2021-10-09T00:00:00+00:00</published><updated>2021-10-09T00:00:00+00:00</updated><id>https://ksheersaagr.github.io/blog/2021/10/suckerbug-python-script-to-scrape-photos-from-facebook-public-pages</id><content type="html" xml:base="https://ksheersaagr.github.io/blog/2021/10/suckerbug-python-script-to-scrape-photos-from-facebook-public-pages/"><![CDATA[<p>P.S. The recent facebook outage grabbed my attention hence I wrote the script for fun ;p</p>

<h2 id="install-the-required-libraries-by-entering-the-commands-in-the-command-terminal-given-below">Install the required libraries by entering the commands in the command terminal given below:</h2>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install facebook-scraper
pip install Pillow
pip install requests
</code></pre></div></div>
<ul>
  <li>
    <p><a href="https://github.com/kevinzg/facebook-scraper">facebook-scraper</a> is a great library with which you can do much more than just scrapping photos from facebook.</p>
  </li>
  <li>
    <p><a href="https://github.com/python-pillow/Pillow">Pillow</a> is a python imaging library.</p>
  </li>
  <li>
    <p><a href="https://github.com/psf/requests">Requests</a> is an HTTP request library used to process URLs.</p>
  </li>
</ul>

<h2 id="importing-packages">Importing packages</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from facebook_scraper import *
import os
import io
import requests
from PIL import Image
import tempfile
</code></pre></div></div>
<ul>
  <li>Now I wanted to make a function that will process the HTTP request, iterate through the page contents and return images that will be stored in the temporary folder(in my case I’ve saved it in the current directory).</li>
</ul>

<h2 id="download-image-function">Download image function</h2>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">download_image</span><span class="p">(</span><span class="n">img_url</span><span class="p">,</span> <span class="n">out_dir</span><span class="p">,</span> <span class="n">img_name</span><span class="p">):</span>
    <span class="n">buffer</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="no">SpooledTemporaryFile</span><span class="p">(</span><span class="n">max_size</span><span class="o">=</span><span class="mf">1e9</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">img_url</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="no">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">r</span><span class="p">.</span><span class="nf">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
        <span class="n">downloaded</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">filesize</span> <span class="o">=</span> <span class="n">int</span><span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="nf">headers</span><span class="p">[</span><span class="s1">'content-length'</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">chunk</span> <span class="k">in</span> <span class="n">r</span><span class="p">.</span><span class="nf">iter_content</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">):</span>
            <span class="n">downloaded</span> <span class="o">+=</span> <span class="n">len</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
            <span class="n">buffer</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">downloaded</span><span class="o">/</span><span class="n">filesize</span><span class="p">)</span>
        <span class="n">buffer</span><span class="p">.</span><span class="nf">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">=</span> <span class="no">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="no">BytesIO</span><span class="p">(</span><span class="n">buffer</span><span class="p">.</span><span class="nf">read</span><span class="p">()))</span>
        <span class="n">i</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="nf">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="n">img_name</span><span class="p">),</span> <span class="n">quality</span><span class="o">=</span><span class="mi">85</span><span class="p">)</span>
    <span class="n">buffer</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span></code></pre></figure>

<h2 id="calling-the-inbuilt-get_photos-function-from-facebook-scraper-library">Calling the inbuilt get_photos() function from facebook-scraper library</h2>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">img</span> <span class="k">in</span> <span class="n">get_photos</span><span class="p">(</span><span class="s1">'name of the public page without spaces'</span><span class="p">,</span> <span class="n">pages</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">img_name</span> <span class="o">=</span> <span class="s2">""</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="s1">'image'</span><span class="p">,</span> <span class="n">str</span><span class="p">(</span><span class="n">count</span><span class="p">),</span> <span class="s1">'.jpg'</span><span class="p">])</span>
    <span class="n">download_image</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="s1">'images'</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="s2">""</span><span class="p">,</span> <span class="n">img_name</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span></code></pre></figure>

<p>Done.</p>

<p><strong>Please find the script on <a href="https://github.com/ksheersaagr/Suckerbug">Github</a>.</strong></p>

<p>Thanks for reading.</p>]]></content><author><name>krunal kshirsagar</name></author><summary type="html"><![CDATA[P.S. The recent facebook outage grabbed my attention hence I wrote the script for fun ;p]]></summary></entry><entry><title type="html">[Idea]-Making autonomous vehicles robust with active learning, federated learning &amp;amp; V2X communication</title><link href="https://ksheersaagr.github.io/blog/2021/04/making-autonomous-vehicles-robust-with-active-learning-federated-learning-v2x-communication/" rel="alternate" type="text/html" title="[Idea]-Making autonomous vehicles robust with active learning, federated learning &amp;amp; V2X communication" /><published>2021-04-14T00:00:00+00:00</published><updated>2021-04-14T00:00:00+00:00</updated><id>https://ksheersaagr.github.io/blog/2021/04/making-autonomous-vehicles-robust-with-active-learning-federated-learning-v2x-communication</id><content type="html" xml:base="https://ksheersaagr.github.io/blog/2021/04/making-autonomous-vehicles-robust-with-active-learning-federated-learning-v2x-communication/"><![CDATA[<p>Recent advances in various machine learning techniques have propelled the enhancement of the autonomous vehicles’ industry. The idea is to couple active learning with federated learning via., v2x communication, to enhance the training of machine learning models. In the case of autonomous vehicles, we almost assume that the roads will be straightforward, flat without any potholes(uncertainty), but that isn’t the case every time. Here, active learning tends to selectively choose the data points with highest informative-ness in order to label them and subsequently add them to the training pool. Additionally, with federated learning we could train our model on multiple agents in a decentralised manner on the local data that we will get from each vehicle to recognise those potholes and help try to save the vehicle from similar potholes. Also, when we think of driving in general, there are good drivers and bad drivers.</p>

<!--more-->

<p>So, on a 2D spectrum, we would picture a cluster of data of those drivers and realise that the good drivers’ data is clustered around a particular coordinate(x,y) on a plane while the bad drivers’ data is all over the place. Because there are numerous characteristics of bad driving but as far as good driving is concerned, they are disciplined and limited. Further, we could train our model to learn from the data that was generated by active learning. Consequently, with federated learning, we can update our model’s parameters across multiple agent vehicles .</p>

<h2 id="introduction">Introduction</h2>
<p>Autonomous vehicles possess a complex system. The system is an amalgamation of many technologies including perception, prediction, sensing, localisation, mapping, interactions with cloud platforms and data storage. With the advent of autonomous vehicles and organisations looking forward to achieving level 5 autonomy, we have to shift our attention towards data, edge-to-edge computing and better machine learning models to make autonomous vehicles robust and safer for smooth navigation from one part of the world to another, while processing the data and simultaneously preserving the privacy wirelessly.</p>

<h2 id="data">Data</h2>
<p>In the new age of technology, data is the holy grail that every organisation is pursuing. Data labelling is a crucial and hectic task when working with computer vision data. The data-that is collected from self-driving cars is in abundance and it keeps growing after each driving test. Hence, manually labelling the data is not scalable. In computer vision, such kind of labelling task can involve drawing bounding boxes around objects and/or segmenting the objects within an image. Thus, the aspect of safety in autonomous vehicles and performance of the deep neural networks is directly dependent on labelling the data correctly to classify the objects perfectly in an image and make decisions based on the result of the network model. Incorrectly labelling the data can degrade the performance of the model.</p>

<p><strong>But not every other data is important and how can we figure out which data matters the most?</strong></p>

<h2 id="active-learning">Active Learning</h2>
<p>Active learning is a training data selection method that actively chooses a small subset of data points that the algorithm wants to learn from using a selection query method for training this diverse data. In the object detection and recognition task, the automatic selection process initialises with training a dedicated deep neural net on a predetermined labelled dataset, then the network sorts through unlabelled data that selects the frames it doesn’t recognise. Additionally, it doesn’t just look for frames with people, potholes, or vehicle objects but where it is most uncertain about those classes. e.g., frames that do not contain a bicycle on the road but is fixed on the back of a truck or car which can confuse the model.</p>

<p><strong>To understand how active learning will work with autonomous vehicles we can look at some examples:</strong></p>

<ul>
  <li>
    <h1 id="good-drivers-and-bad-drivers">Good Drivers and Bad Drivers</h1>
    <p>Let’s take task of changing lanes by autonomous vehicles for e.g., when a vehicle needs to change its lane in order to overtake the vehicle cruising in front of it, also keeping in check of the vehicle behind it and on the next lane as well. There are so many parameters that a vehicle has to check in order to make that lane change smoothly. Now, if the model learns a lane change from data that says to change the lane that is nice, but there is a problem in acquisition of such data where some humans do what may be characterized as bad lane changes. <strong><em><a href="https://www.theverge.com/2016/6/6/11866868/comma-ai-george-hotz-interview-self-driving-cars">All good drivers are good in the same way and all bad drivers are bad in different ways- George Hotz, Comma.ai.</a></em></strong> <strong>( might have derived from the <a href="https://en.wikipedia.org/wiki/Anna_Karenina">Anna Karenina principle</a> by Leo Tolstoy )</strong> Now, hypothetically the task is to discover the good drivers on the 2D spectrum. The good drivers stand out because they’re in one cluster, and the bad drivers are scattered all over the place and your network’s task is to learn from the cluster of good drivers. They’re easy to cluster, our model learned from all of them and that automatically learns the policy of the majority. Because driving is context-dependent and depends on the visual scene and not that mainly on drivers. Let’s assume there is more than one cluster present on the plane and others are random noise and probably bad, now which one of those clusters will you choose to learn from? The problem lies with the bad drivers. How far are they from being good drivers? Here, active learning can learn from the most uncertain of the drivers, it will look for the hard negative samples from the cluster of data and try and fit the model.</p>
  </li>
  <li>
    <h1 id="outlier-detection">Outlier Detection</h1>
    <p>Outliers are the anomalies that a system comes across when training machine learning models. In simple words, outliers are exception samples that have less likelihood than a specified threshold. Thus, the removal of outliers can be the most common thought here. But, as a result, it can lead to overfit a model. Instead, just as we learn from our mistakes, outliers can prove to be a vital source of information about the data which the model is most uncertain about, such that we rectify our data training pool. Consequently, making a robust classification process.</p>
  </li>
</ul>

<h1 id="steps-to-perform-the-desired-machine-learning-task">Steps to perform the desired machine learning task:</h1>
<p><img src="/blog/images/2021-04-14-making-autonomous-vehicles-robust-with-active-learning-federated-learning-v2x-communication-active-learning.svg " />
<strong><em>1. Initialise the Active Learner(Query Selector).</em></strong><br />
<strong><em>2. The learner then predicts the informativeness of each data point from the unlabelled data pool and chooses the most informative unlabelled sample.</em></strong><br />
<strong><em>3. An oracle labels the query data and adds them to the training dataset.</em></strong> <br />
<strong><em>4. The learner is re-trained on the updated dataset, that’s one iteration.</em></strong><br />
<strong><em>5. Repeat the steps with other rounds of informativeness estimation.</em></strong></p>

<p><strong>But how can we compute, process the data and federate the updated parameters across multiple autonomous vehicles?</strong></p>

<h2 id="autonomous-vehicles-as-edge-devices">Autonomous Vehicles as Edge Devices</h2>

<p>Edge computing presents a solid foundation for computing and processing the data in a distributed manner. Therefore, to process the data and train the model locally, we can use autonomous vehicles as edge devices and send relevant data as well as update parameters back-and-forth from autonomous vehicles(local models) to the central cloud(global model) respectively.</p>

<h2 id="federated-learning">Federated Learning</h2>

<p>Federated learning is a machine learning technique that utilises the power of edge computing. Federated learning makes the model learn collaboratively in a decentralised manner while keeping all the data at the edge device itself. In simple words, <strong><em>”it doesn’t take the data to where the training algorithm is; rather, bringing the algorithm to where the data is.”</em></strong> The biggest advantage of federated learning is that you do not have to share your data instead, the model is trained upon your data on your device itself and the parameters are updated accordingly, further aggregating it with other user’s updates to improve the shared model. Also, in federated learning, it is not necessary to have the data on a centralized server but, we have to use these tools according to our use case. <strong>In order to send the data back and forth, perform federated learning and update the model across multiple vehicles at a faster rate, there has to be a robust channel in between.</strong></p>

<h4 id="federated-learning-over-wireless-communication">Federated Learning over Wireless Communication?</h4>

<h2 id="v2x-communication">V2X Communication</h2>

<p>The rise in wireless communication technology such as fifth-generation (5G) networks and sixth-generation (6G) networks paves the way for transferring huge chunks of data wirelessly without any packet loss. Therefore, this lower latency and higher bandwidth communication network builds a solid base for transferring data with 5G and 6G architectures at its core. V2X is a vehicle communication system that consists of many types of communications: vehicle-to-vehicle (V2V), vehicle-to-infrastructure(V2I), vehicle-to-network (V2N), vehicle-to-pedestrian (V2P), vehicle-to-device (V2D), and vehicle-to-grid (V2G).</p>
<ul>
  <li><strong>Let’s take an example of potholes:</strong> Whichever vehicle has identified the pothole, avoided it, and drove around it or could have slowed down the vehicle, instead of hitting into it. That vehicle’s data and model are sent to the cloud over the network channel, where the active learner classifies the data as good driving data, and the data is then added to the training pool. Further, the model is aggregated, trained on that good driving data, and the updated parameters are then sent to the local model thereby, updating the local parameters.</li>
</ul>

<h2 id="procedure">Procedure</h2>

<p><img src="/blog/images/2021-04-14-making-autonomous-vehicles-robust-with-active-learning-federated-learning-v2x-communication-procedure-1-3.svg " /></p>

<p><strong><em>1. Initially, a model is trained on the global dataset and a copy of that model is maintained across multiple autonomous vehicles.</em></strong><br />
<strong><em>2. Driving through the city, the autonomous vehicles trains the local model on the local data that they’ve gathered and sent back the local data as well as federate the updated local machine learning models’ parameters to the cloud simultaneously at a regular interval.</em></strong><br />
<strong><em>3. On the cloud, active learning is performed on the gathered dataset and a subset of relevant and informative data is extracted and added to the training data pool.</em></strong></p>

<p><img src="/blog/images/2021-04-14-making-autonomous-vehicles-robust-with-active-learning-federated-learning-v2x-communication-procedure-4-6.svg " /></p>

<p><strong><em>4. Next, the local machine learning models are aggregated and the global model is updated.</em></strong><br />
<strong><em>5. Later, The global model is then trained on the subset of informative data that is extracted by active learning with the help of an oracle.</em></strong><br />
<strong><em>6. Lastly, the global model parameters are federated across multiple autonomous vehicles and the local machine learning models’ parameters are updated.</em></strong></p>

<h2 id="conclusion">Conclusion</h2>

<p>The idea is to train the model locally on the local data as well as globally on a training pool- which was generated by performing active learning on the gathered data from multiple autonomous vehicles. Further, update the parameters(weights and biases) of the model and distribute them across multiple autonomous vehicles. Whilst these techniques seems promising, they have their complications concerning security and data protection.
Nowadays, federated learning is being sold upon the notion of security and privacy but in federated learning the models can be tampered with back-door attacks and data poisoning; might as well threaten the model by performing gradient updates leading to model-poisoning. Data compression is necessary for federated learning for the smooth, faster and secure transmission of data over the network. Although it’s necessary for organisation to have essential data that is needed for the data-hungry model for it to be trained upon, the individual drivers need to have some autonomy over their data - such that the individuals should have a free will to choose what <strong>’kind’</strong> of data they need to share with the respective organisation.Also, with edge computing, it’s important to have reasonable energy consumption with sufficient computing power and as far as v2x communication is concerned, it is in the early stages, tackling the problems such as latency, bandwidth, reliability of the network issues and security from various attacks like DoS and distributed DoS (DDoS) attacks. Although performing federated learning and updating parameters over v2x seem possible, the parameters need to be quantised before sending over the network. Parameter quantisation will lead to the robustness of model from quantisation error. The wireless channel quality should be considered for convergence time of the model - that includes the computation time on local edge devices and the global aggregator plus the communication time in-between them. To reduce the complexity of the model and scale down model parameters, it is necessary to practise model compression and sparse training approaches over the network while maintaining the accuracy of the model. We have to think about communication cost and quality of wireless channel for model optimisation over wireless communication.</p>

<h2 id="references">References</h2>

<ol>
  <li><a href="https://www.diva-portal.org/smash/get/diva2:1415945/FULLTEXT01.pdf">What Is Active learning?</a></li>
  <li><a href="https://ai.googleblog.com/2017/04/federated-learning-collaborative.html">Federated learning: collaborative machine learning without centralised training data</a></li>
  <li><a href="https://www.thalesgroup.com/en/markets/digital-identity-and-security/iot/industries/automotive/use-cases/v2x">V2X: What is vehicle to everything?</a></li>
  <li><a href="https://www.datacamp.com/community/tutorials/active-learning">Active learning: curious AI algorithms</a></li>
  <li><a href="https://medium.com/nvidia-ai/scalable-active-learning-for-autonomous-driving-a-practical-implementation-and-a-b-test-4d315ed04b5f">Scalable active learning for autonomous driving</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Anna_Karenina">Anna Karenina principle</a></li>
  <li><a href="https://www.researchgate.net/publication/46608952_Bad_Is_Stronger_than_Good">Bad is stronger than good</a></li>
  <li><a href="https://www.researchgate.net/publication/221653343_Outlier_detection_by_active_learning">Outlier detection by active learning</a></li>
  <li><a href="https://www.networkworld.com/article/3224893/what-is-edge-computing-and-how-it-s-changing-the-network.html">What is edge computing and why it matters?</a></li>
  <li><a href="http://weisong.eng.wayne.edu/_resources/pdfs/liu19-EdgeAV.pdf">Edge computing for autonomous driving: opportunities and challenges</a></li>
  <li><a href="https://www.aitrends.com/selfdrivingcars/edge-computing-ai-self-driving-cars/">Edge computing for AI self-driving cars</a></li>
  <li><a href="https://www.aitrends.com/ai-insider/federated-machine-learning-for-ai-self-driving-cars/">Federated machine learning for AI self-driving cars</a></li>
  <li><a href="https://arxiv.org/pdf/1908.06847.pdf">Federated learning for wireless communications: motivation, opportunities and challenges</a></li>
  <li><a href="https://arxiv.org/ftp/arxiv/papers/2006/2006.06091.pdf">Autonomous driving with deep learning: a survey of state-of-art technologies</a></li>
</ol>

<p>Thanks for reading.</p>]]></content><author><name>krunal kshirsagar</name></author><summary type="html"><![CDATA[Recent advances in various machine learning techniques have propelled the enhancement of the autonomous vehicles’ industry. The idea is to couple active learning with federated learning via., v2x communication, to enhance the training of machine learning models. In the case of autonomous vehicles, we almost assume that the roads will be straightforward, flat without any potholes(uncertainty), but that isn’t the case every time. Here, active learning tends to selectively choose the data points with highest informative-ness in order to label them and subsequently add them to the training pool. Additionally, with federated learning we could train our model on multiple agents in a decentralised manner on the local data that we will get from each vehicle to recognise those potholes and help try to save the vehicle from similar potholes. Also, when we think of driving in general, there are good drivers and bad drivers.]]></summary></entry><entry><title type="html">Graph SLAM: A Noob’s Guide to Simultaneous Localisation and Mapping</title><link href="https://ksheersaagr.github.io/blog/2020/03/graph-slam-a-noobs-guide-to-simultaneous-localisation-and-mapping/" rel="alternate" type="text/html" title="Graph SLAM: A Noob’s Guide to Simultaneous Localisation and Mapping" /><published>2020-03-31T00:00:00+00:00</published><updated>2020-03-31T00:00:00+00:00</updated><id>https://ksheersaagr.github.io/blog/2020/03/graph-slam-a-noobs-guide-to-simultaneous-localisation-and-mapping</id><content type="html" xml:base="https://ksheersaagr.github.io/blog/2020/03/graph-slam-a-noobs-guide-to-simultaneous-localisation-and-mapping/"><![CDATA[<p>Simultaneous localisation and mapping (SLAM) used in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. In other words, SLAM gives you a way to track the location of a robot in the world in real-time and identify the locations of landmarks such as buildings, trees, rocks, and other world features. In addition to localisation, we also want to build up a model of the robot’s environment so that we have an idea of objects, and landmarks that surround it and so that we can use this map data to ensure that we are on the right path as the robot moves through the world. So the key insight in building a map is that the robot itself might lose track of where it is by virtue of its motion uncertainty since there is no presence of an existing map because we are building the map simultaneously. That’s where SLAM comes into play.</p>

<h1 id="working-of-slam">Working of SLAM:</h1>

<p>The basis for simultaneous localisation and mapping (SLAM) is to gather information from a robot’s sensors and motions over time, and then use information about measurements and motion to reconstruct a map of the world. In this case, we’ll be localizing a robot in a 2D grid world and therefore, a <strong><a href="http://www2.informatik.uni-freiburg.de/~stachnis/pdf/grisetti10titsmag.pdf">graph-based SLAM approach constructs a simplified estimation problem by abstracting the raw sensor measurements. These raw measurements are replaced by the edges in the graph which can then be seen as virtual measurements</a>.</strong><br />
Let’s assume we have a robot and the initial location, <strong>x0=0 &amp; y0=0.</strong> For this example, we don’t care about heading direction just to keep things simple. Let’s assume the robot moves to the right in the X-direction by <strong>10</strong>. So, In a perfect world, you would know that <strong>x1</strong>, the location after motion is the same as <strong>x0+10</strong> in other words, <strong>x1=x0+10</strong>, and <strong>y1</strong> is the same as <strong>y0.</strong></p>

<p><img src="/blog/images/2020-03-31-graph-slam-a-noobs-guide-to-simultaneous-localisation-and-mapping-displacement-by-10.png " /></p>

<p>But according to <strong><a href="https://ksheersaagr.github.io/blog/2020/02/the-curious-case-of-kalman-filters/">Kalman filters</a></strong> and various other robotic techniques, we have learned that the <strong>location is actually uncertain</strong>. So, rather than assuming in our X-Y coordinate system the robot moved to the right by 10 exactly, it’s better to understand that the actual location of the robot after the x1= x0+10 motion update is a Gaussian centered around (10,0), but it’s possible that the robot is somewhere else.</p>

<p><img src="/blog/images/2020-03-31-graph-slam-a-noobs-guide-to-simultaneous-localisation-and-mapping-gaussian-centered-around-the-location-of-robot-after-motion-update.png " /></p>

<h1 id="heres-the-math-for-the-gaussian-of-x-variable">Here’s the math for the Gaussian of x variable:</h1>

<p>Rather than setting x1 to x0+10, let’s express it in Gaussian that peaks when these two things are the same. So, if you subtract x1-x0-10, put this into a square format, and turn this into a Gaussian, we get a probability distribution that relates x1 and x0. We can do the same for y. Since there is no change in y according to our motion, y1 &amp; y0 are as close together as possible.</p>

<p><img src="/blog/images/2020-03-31-graph-slam-a-noobs-guide-to-simultaneous-localisation-and-mapping-likelihood.png " /></p>

<p>The product of these two Gaussian is now our constraint. The goal is to maximize the likelihood of the position x1 given the position x0 is (0,0). <strong>So, what graph-SLAM does is, it defines the probabilities using a sequence of such constraints.</strong> Say we have a robot that moves in some space, GRAPH SLAM collects its initial location which is (0,0) initially, also called as <strong>Initial Constraints</strong>, then collects lots of relative constraints that relate each robot pose to the previous robot pose also called as <strong>Relative Motion Constraints.</strong> As an example, let’s use landmarks that can be seen by the robot at various locations which would be <strong>Relative Measurement Constraints</strong> every time a robot sees a landmark. <strong>So, Graph SLAM collects those constraints in order to find the most likely configuration of the robot path along with the location of landmarks, and that is the mapping process.</strong></p>

<p><img src="/blog/images/2020-03-31-graph-slam-a-noobs-guide-to-simultaneous-localisation-and-mapping-update-position.png " /></p>

<h2 id="implementation">Implementation</h2>

<h1 id="generating-an-environment">Generating an environment:</h1>
<p>We will generate a 2D world grid with landmarks in it and then generate data by placing a robot in that world and moving and sensing over some number of time steps. The data is collected as an instantiated robot moves and senses in a world. Our SLAM function will take in this data as input. So, let’s first create this data and explore how it represents the movement and sensor measurements that our robot takes.</p>

<h1 id="slam-inputs">SLAM inputs:</h1>

<h4 id="in-addition-to-data-our-slam-function-takes-in">In addition to data, our slam function takes in:</h4>

<ul>
  <li><strong>N:</strong> The number of time steps that a robot will be moving and sensing.</li>
  <li><strong>num_landmarks:</strong> The number of landmarks in the world.</li>
  <li><strong>world_size:</strong> The size (w/h) of your world.</li>
  <li><strong>motion_noise:</strong> The noise associated with motion; the update confidence for motion should be <code class="highlighter-rouge">1.0/motion_noise</code>.</li>
  <li><strong>measurement_noise:</strong> The noise associated with measurement/sensing; the update weight for measurement should be <code class="highlighter-rouge">1.0/measurement_noise</code>.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">helpers</span> <span class="kn">import</span> <span class="n">make_data</span>

<span class="c1"># your implementation of slam should work with the following inputs
# feel free to change these input values and see how it responds!
</span>
<span class="c1"># world parameters
</span><span class="n">num_landmarks</span>      <span class="o">=</span> <span class="mi">5</span>        <span class="c1"># number of landmarks
</span><span class="n">N</span>                  <span class="o">=</span> <span class="mi">20</span>       <span class="c1"># time steps
</span><span class="n">world_size</span>         <span class="o">=</span> <span class="mf">100.0</span>    <span class="c1"># size of world (square)
</span>
<span class="c1"># robot parameters
</span><span class="n">measurement_range</span>  <span class="o">=</span> <span class="mf">50.0</span>     <span class="c1"># range at which we can sense landmarks
</span><span class="n">motion_noise</span>       <span class="o">=</span> <span class="mf">2.0</span>      <span class="c1"># noise in robot motion
</span><span class="n">measurement_noise</span>  <span class="o">=</span> <span class="mf">2.0</span>      <span class="c1"># noise in the measurements
</span><span class="n">distance</span>           <span class="o">=</span> <span class="mf">20.0</span>     <span class="c1"># distance by which robot (intends to) move each iteratation 
</span>

<span class="c1"># make_data instantiates a robot, AND generates random landmarks for a given world size and number of landmarks
</span><span class="n">data</span> <span class="o">=</span> <span class="n">make_data</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">measurement_range</span><span class="p">,</span> <span class="n">motion_noise</span><span class="p">,</span> <span class="n">measurement_noise</span><span class="p">,</span> <span class="n">distance</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="lets-write-our-two-main-functions-that-move-our-robot-around-help-locate-landmarks-and-measure-the-range-between-them-on-a-2d-map">Let’s write our two main functions that move our robot around, help locate landmarks and measure the range between them on a 2D map:</h1>

<ul>
  <li><strong>Move:</strong> attempts to move the robot by dx, dy.
Sense: returns x and y distances to landmarks within the visibility range.</li>
  <li><strong>Sense:</strong> returns x and y distances to landmarks within the visibility range.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">robot</span><span class="p">:</span>
    
    <span class="c1">#move function
</span>    <span class="k">def</span> <span class="nf">move</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">):</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">dx</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">motion_noise</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">dy</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">motion_noise</span>
        
        <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mf">0.0</span> <span class="ow">or</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">world_size</span> <span class="ow">or</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="mf">0.0</span> <span class="ow">or</span> <span class="n">y</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">world_size</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
            <span class="k">return</span> <span class="bp">True</span>
    
    
    <span class="c1">#sense function
</span>    <span class="k">def</span> <span class="nf">sense</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">measurements</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">landmark_index</span><span class="p">,</span> <span class="n">landmark</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">landmarks</span><span class="p">):</span>
            <span class="n">landmark_distance_x</span> <span class="o">=</span> <span class="n">landmark</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">landmark_distance_y</span> <span class="o">=</span> <span class="n">landmark</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">random_noise</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rand</span><span class="p">()</span>
            <span class="n">cal_dx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">landmark_distance_x</span> <span class="o">+</span> <span class="n">random_noise</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">measurement_noise</span>
            <span class="n">cal_dy</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="n">landmark_distance_y</span> <span class="o">+</span> <span class="n">random_noise</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">measurement_noise</span>
            <span class="n">is_not_in_measurement_range</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">measurement_range</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">if</span><span class="p">(</span><span class="n">is_not_in_measurement_range</span><span class="p">)</span> <span class="ow">or</span> <span class="p">((</span><span class="nb">abs</span><span class="p">(</span><span class="n">cal_dx</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">measurement_range</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">cal_dy</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">measurement_range</span><span class="p">)):</span>
                <span class="n">measurements</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">landmark_index</span><span class="p">,</span> <span class="n">cal_dx</span><span class="p">,</span> <span class="n">cal_dy</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">measurements</span>
</code></pre></div></div>

<h1 id="omega-and-xi">Omega and Xi:</h1>
<p>To implement Graph SLAM, a matrix and a vector (omega and xi, respectively) are introduced. The matrix is square, labeled with all the robot poses (xi) and all the landmarks. Every time you make an observation, for example, as you move between two poses by some distance dx and can relate those two positions, you can represent this as a numerical relationship in these matrices. let’s write the function such that it returns omega and xi constraints for the starting position of the robot. Any values that we do not yet know should be initialized with the value 0. we may assume that our robot starts out in exactly the middle of the world with 100% confidence.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">initialize_constraints</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="s">''' This function takes in a number of time steps N, number of landmarks, and a world_size,
        and returns initialized constraint matrices, omega and xi.'''</span>
    
    <span class="n">middle_of_the_world</span> <span class="o">=</span> <span class="n">world_size</span> <span class="o">/</span> <span class="mi">2</span>
    
    <span class="c1">## Recommended: Define and store the size (rows/cols) of the constraint matrix in a variable
</span>    <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">num_landmarks</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">num_landmarks</span><span class="p">)</span>
    <span class="c1">## TODO: Define the constraint matrix, Omega, with two initial "strength" values
</span>    <span class="n">omega</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">))</span>
    <span class="c1">## for the initial x, y location of our robot
</span>    <span class="c1">#omega = [0]
</span>    
    <span class="n">omega</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">omega</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span><span class="mi">1</span>
    
    <span class="c1">## TODO: Define the constraint *vector*, xi
</span>    <span class="c1">## you can assume that the robot starts out in the middle of the world with 100% confidence
</span>    <span class="c1">#xi = [0]
</span>    <span class="n">xi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">xi</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">middle_of_the_world</span>
    <span class="n">xi</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">middle_of_the_world</span>
    
    <span class="k">return</span> <span class="n">omega</span><span class="p">,</span> <span class="n">xi</span>
</code></pre></div></div>

<h1 id="updating-with-motion-and-measurements">Updating with motion and measurements:</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## slam takes in 6 arguments and returns mu, 
## mu is the entire path traversed by a robot (all x,y poses) *and* all landmarks locations
</span><span class="k">def</span> <span class="nf">slam</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">motion_noise</span><span class="p">,</span> <span class="n">measurement_noise</span><span class="p">):</span>
    
    <span class="c1">## TODO: Use your initilization to create constraint matrices, omega and xi
</span>    <span class="n">omega</span><span class="p">,</span> <span class="n">xi</span> <span class="o">=</span> <span class="n">initialize_constraints</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
    <span class="c1">## TODO: Iterate through each time step in the data
</span>    <span class="k">for</span> <span class="n">time_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)):</span>
        
        <span class="c1">## get all the motion and measurement data as you iterate through each time step
</span>        <span class="n">measurement</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">time_step</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">motion</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">time_step</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="n">dx</span> <span class="o">=</span> <span class="n">motion</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>         <span class="c1"># distance to be moved along x in this time_step
</span>        <span class="n">dy</span> <span class="o">=</span> <span class="n">motion</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>         <span class="c1"># distance to be moved along y in this time_step
</span>        
        <span class="c1">#Consider the robot moves from (x0,y0) to (x1,y1) in this time_step
</span>        
        <span class="c1">#even numbered columns of omega correspond to x values
</span>        <span class="n">x0</span> <span class="o">=</span> <span class="p">(</span><span class="n">time_step</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>   <span class="c1">#x0 = 0,2,4,...
</span>        <span class="n">x1</span> <span class="o">=</span> <span class="n">x0</span> <span class="o">+</span> <span class="mi">2</span>            <span class="c1">#x1 = 2,4,6,...
</span>        
        <span class="c1">#odd numbered columns of omega correspond to y values
</span>        <span class="n">y0</span> <span class="o">=</span> <span class="n">x0</span> <span class="o">+</span> <span class="mi">1</span>            <span class="c1">#y0 = 1,3,5,...
</span>        <span class="n">y1</span> <span class="o">=</span> <span class="n">y0</span> <span class="o">+</span> <span class="mi">2</span>            <span class="c1">#y1 = 3,5,7,...
</span>        
        <span class="n">actual_m_noise</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="n">measurement_noise</span>
        <span class="n">actual_n_noise</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="n">motion_noise</span>
    <span class="c1">## TODO: update the constraint matrix/vector(omega/xi) to account for all *measurements*
</span>    <span class="c1">## this should be a series of additions that take into account the measurement noise
</span>        <span class="k">for</span> <span class="n">landmark</span> <span class="ow">in</span> <span class="n">measurement</span><span class="p">:</span>
            <span class="n">lM</span> <span class="o">=</span> <span class="n">landmark</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>            <span class="c1"># landmark id
</span>            <span class="n">dx_lM</span> <span class="o">=</span> <span class="n">landmark</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>         <span class="c1"># separation along x from current position
</span>            <span class="n">dy_lM</span> <span class="o">=</span> <span class="n">landmark</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>         <span class="c1"># separation along y from current position
</span>            
            <span class="n">L_x0</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">lM</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>       <span class="c1"># even-numbered columns have x values of landmarks
</span>            <span class="n">L_y0</span> <span class="o">=</span> <span class="n">L_x0</span> <span class="o">+</span> <span class="mi">1</span>             <span class="c1"># odd-numbered columns have y values of landmarks
</span>
            <span class="c1"># update omega values corresponding to measurement between x0 and Lx0
</span>            <span class="n">omega</span><span class="p">[</span><span class="n">x0</span><span class="p">][</span><span class="n">x0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">actual_m_noise</span>
            <span class="n">omega</span><span class="p">[</span><span class="n">L_x0</span><span class="p">][</span><span class="n">L_x0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">actual_m_noise</span>
            <span class="n">omega</span><span class="p">[</span><span class="n">x0</span><span class="p">][</span><span class="n">L_x0</span><span class="p">]</span> <span class="o">+=</span> <span class="o">-</span><span class="n">actual_m_noise</span>
            <span class="n">omega</span><span class="p">[</span><span class="n">L_x0</span><span class="p">][</span><span class="n">x0</span><span class="p">]</span> <span class="o">+=</span> <span class="o">-</span><span class="n">actual_m_noise</span>
            
            <span class="c1"># update omega values corresponding to measurement between y0 and Ly0
</span>            <span class="n">omega</span><span class="p">[</span><span class="n">y0</span><span class="p">][</span><span class="n">y0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">actual_m_noise</span>
            <span class="n">omega</span><span class="p">[</span><span class="n">L_y0</span><span class="p">][</span><span class="n">L_y0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">actual_m_noise</span>
            <span class="n">omega</span><span class="p">[</span><span class="n">y0</span><span class="p">][</span><span class="n">L_y0</span><span class="p">]</span> <span class="o">+=</span> <span class="o">-</span><span class="n">actual_m_noise</span>
            <span class="n">omega</span><span class="p">[</span><span class="n">L_y0</span><span class="p">][</span><span class="n">y0</span><span class="p">]</span> <span class="o">+=</span> <span class="o">-</span><span class="n">actual_m_noise</span>
            
            <span class="c1"># update xi values corresponding to measurement between x0 and Lx0
</span>            <span class="n">xi</span><span class="p">[</span><span class="n">x0</span><span class="p">]</span>  <span class="o">-=</span> <span class="n">dx_lM</span><span class="o">/</span><span class="n">measurement_noise</span>
            <span class="n">xi</span><span class="p">[</span><span class="n">L_x0</span><span class="p">]</span>  <span class="o">+=</span> <span class="n">dx_lM</span><span class="o">/</span><span class="n">measurement_noise</span>
            
            <span class="c1"># update xi values corresponding to measurement between y0 and Ly0
</span>            <span class="n">xi</span><span class="p">[</span><span class="n">y0</span><span class="p">]</span>  <span class="o">-=</span> <span class="n">dy_lM</span><span class="o">/</span><span class="n">measurement_noise</span>
            <span class="n">xi</span><span class="p">[</span><span class="n">L_y0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">dy_lM</span><span class="o">/</span><span class="n">measurement_noise</span>
            
            
        <span class="c1">## TODO: update the constraint matrix/vector(omega/xi) to account for all *motion* from from (x0,y0) to (x1,y1) and motion noise
</span>        <span class="n">omega</span><span class="p">[</span><span class="n">x0</span><span class="p">][</span><span class="n">x0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">actual_n_noise</span>
        <span class="n">omega</span><span class="p">[</span><span class="n">x1</span><span class="p">][</span><span class="n">x1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">actual_n_noise</span>
        <span class="n">omega</span><span class="p">[</span><span class="n">x0</span><span class="p">][</span><span class="n">x1</span><span class="p">]</span> <span class="o">+=</span> <span class="o">-</span><span class="n">actual_n_noise</span>
        <span class="n">omega</span><span class="p">[</span><span class="n">x1</span><span class="p">][</span><span class="n">x0</span><span class="p">]</span> <span class="o">+=</span> <span class="o">-</span><span class="n">actual_n_noise</span>
        
        <span class="n">omega</span><span class="p">[</span><span class="n">y0</span><span class="p">][</span><span class="n">y0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">actual_n_noise</span>
        <span class="n">omega</span><span class="p">[</span><span class="n">y1</span><span class="p">][</span><span class="n">y1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">actual_n_noise</span>
        <span class="n">omega</span><span class="p">[</span><span class="n">y0</span><span class="p">][</span><span class="n">y1</span><span class="p">]</span> <span class="o">+=</span> <span class="o">-</span><span class="n">actual_n_noise</span>
        <span class="n">omega</span><span class="p">[</span><span class="n">y1</span><span class="p">][</span><span class="n">y0</span><span class="p">]</span> <span class="o">+=</span> <span class="o">-</span><span class="n">actual_n_noise</span>
        
        <span class="n">xi</span><span class="p">[</span><span class="n">x0</span><span class="p">]</span> <span class="o">-=</span> <span class="n">dx</span><span class="o">/</span><span class="n">motion_noise</span>
        <span class="n">xi</span><span class="p">[</span><span class="n">y0</span><span class="p">]</span> <span class="o">-=</span> <span class="n">dy</span><span class="o">/</span><span class="n">motion_noise</span>
        
        <span class="n">xi</span><span class="p">[</span><span class="n">x1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">dx</span><span class="o">/</span><span class="n">motion_noise</span>
        <span class="n">xi</span><span class="p">[</span><span class="n">y1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">dy</span><span class="o">/</span><span class="n">motion_noise</span>
    
    <span class="c1">## TODO: After iterating through all the data
</span>    <span class="c1">## Compute the best estimate of poses and landmark positions
</span>    <span class="c1">## using the formula, omega_inverse * Xi
</span>    <span class="n">inverse_of_omega</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">omega</span><span class="p">))</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">inverse_of_omega</span> <span class="o">*</span> <span class="n">xi</span>
    
    <span class="k">return</span> <span class="n">mu</span>
</code></pre></div></div>

<h1 id="robot-poses--landmarks">Robot Poses &amp; Landmarks:</h1>

<p>Let’s print the estimated pose and landmark locations that our function has produced. We define a function that extracts the poses and landmarks locations and returns those as their own separate lists.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_poses_landmarks</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
    <span class="c1"># create a list of poses
</span>    <span class="n">poses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">poses</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">mu</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">i</span><span class="p">].</span><span class="n">item</span><span class="p">(),</span> <span class="n">mu</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">].</span><span class="n">item</span><span class="p">()))</span>

    <span class="c1"># create a list of landmarks
</span>    <span class="n">landmarks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_landmarks</span><span class="p">):</span>
        <span class="n">landmarks</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">mu</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">i</span><span class="p">)].</span><span class="n">item</span><span class="p">(),</span> <span class="n">mu</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">].</span><span class="n">item</span><span class="p">()))</span>

    <span class="c1"># return completed lists
</span>    <span class="k">return</span> <span class="n">poses</span><span class="p">,</span> <span class="n">landmarks</span>
  
<span class="k">def</span> <span class="nf">print_all</span><span class="p">(</span><span class="n">poses</span><span class="p">,</span> <span class="n">landmarks</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Estimated Poses:'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">poses</span><span class="p">)):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'['</span><span class="o">+</span><span class="s">', '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="s">'%.3f'</span><span class="o">%</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">poses</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">+</span><span class="s">']'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Estimated Landmarks:'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">landmarks</span><span class="p">)):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'['</span><span class="o">+</span><span class="s">', '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="s">'%.3f'</span><span class="o">%</span><span class="n">l</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">landmarks</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">+</span><span class="s">']'</span><span class="p">)</span>

<span class="c1"># call your implementation of slam, passing in the necessary parameters
</span><span class="n">mu</span> <span class="o">=</span> <span class="n">slam</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">motion_noise</span><span class="p">,</span> <span class="n">measurement_noise</span><span class="p">)</span>

<span class="c1"># print out the resulting landmarks and poses
</span><span class="k">if</span><span class="p">(</span><span class="n">mu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">):</span>
    <span class="c1"># get the lists of poses and landmarks
</span>    <span class="c1"># and print them out
</span>    <span class="n">poses</span><span class="p">,</span> <span class="n">landmarks</span> <span class="o">=</span> <span class="n">get_poses_landmarks</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
    <span class="n">print_all</span><span class="p">(</span><span class="n">poses</span><span class="p">,</span> <span class="n">landmarks</span><span class="p">)</span>
</code></pre></div></div>
<h4 id="estimated-robot-poses-and-landmarks">Estimated Robot poses and landmarks</h4>

<p><img src="/blog/images/2020-03-31-graph-slam-a-noobs-guide-to-simultaneous-localisation-and-mapping-estimated-robot-poses-and-landmarks.png " /></p>

<h1 id="visualise-the-constructed-world">Visualise the constructed world:</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># import the helper function
</span><span class="kn">from</span> <span class="nn">helpers</span> <span class="kn">import</span> <span class="n">display_world</span>

<span class="c1"># Display the final world!
</span>
<span class="c1"># define figure size
</span><span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">"figure.figsize"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># check if poses has been created
</span><span class="k">if</span> <span class="s">'poses'</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">():</span>
    <span class="c1"># print out the last pose
</span>    <span class="k">print</span><span class="p">(</span><span class="s">'Last pose: '</span><span class="p">,</span> <span class="n">poses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># display the last position of the robot *and* the landmark positions
</span>    <span class="n">display_world</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">world_size</span><span class="p">),</span> <span class="n">poses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">landmarks</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="output">Output:</h4>

<p><img src="/blog/images/2020-03-31-graph-slam-a-noobs-guide-to-simultaneous-localisation-and-mapping-lastpose.png " /></p>

<p>Check out the code on <strong><a href="https://github.com/ksheersaagr/Landmark_Detection_Robot_Tracking_SLAM-">Github.</a></strong></p>]]></content><author><name>krunal kshirsagar</name></author><summary type="html"><![CDATA[Simultaneous localisation and mapping (SLAM) used in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. In other words, SLAM gives you a way to track the location of a robot in the world in real-time and identify the locations of landmarks such as buildings, trees, rocks, and other world features. In addition to localisation, we also want to build up a model of the robot’s environment so that we have an idea of objects, and landmarks that surround it and so that we can use this map data to ensure that we are on the right path as the robot moves through the world. So the key insight in building a map is that the robot itself might lose track of where it is by virtue of its motion uncertainty since there is no presence of an existing map because we are building the map simultaneously. That’s where SLAM comes into play.]]></summary></entry><entry><title type="html">The Curious Case of Kalman Filters</title><link href="https://ksheersaagr.github.io/blog/2020/02/the-curious-case-of-kalman-filters/" rel="alternate" type="text/html" title="The Curious Case of Kalman Filters" /><published>2020-02-28T00:00:00+00:00</published><updated>2020-02-28T00:00:00+00:00</updated><id>https://ksheersaagr.github.io/blog/2020/02/the-curious-case-of-kalman-filters</id><content type="html" xml:base="https://ksheersaagr.github.io/blog/2020/02/the-curious-case-of-kalman-filters/"><![CDATA[<p>Kalman filter finds the most optimum averaging factor for each consequent state. Also somehow remembers a little bit about the past states. It performs a joint probability distribution over the variables for each timeframe. The algorithm uses the new mean and new variance for every step in order to calculate the uncertainty and tries to provide accurate measurements for each timeframe of the measurement update (sensing/prediction) and the motion update(moving). The algorithm also uses other inaccuracies and statistical noise in order to represent the initial uncertainty.</p>

<h1 id="purpose-of-kalman-filters">Purpose of Kalman Filters:</h1>

<ul>
  <li><strong>Transform data input from various sensors like LiDAR and Radar trackers into a usable form.</strong></li>
  <li><strong>To Calculate inferring velocity.</strong></li>
  <li><strong>Reduce measurement error(noise) of the target’s position and velocity.</strong></li>
  <li><strong>Predict the future state of the target using previous state estimate and new data.</strong></li>
  <li><strong>Lightweight, Robust and expandable algorithm.</strong></li>
  <li><strong>Estimates a continuous state and as a result, Kalman Filters happens to give us an Uni-modal distribution.</strong></li>
</ul>

<h1 id="working-of-kalman-filters">Working of Kalman Filters:</h1>

<p>A Kalman filter gives us a mathematical way to infer velocity from only a set of measured locations. So, here I’m going to create a 1D Kalman filter that takes in positions, takes into account uncertainty, and estimates where future locations might be and the velocity of an object. Further, if we want to understand how Kalman filter works, we first need to know a bit about Gaussians which represents the Uni-modal distribution in the Kalman filters.</p>

<h1 id="gaussian">Gaussian:</h1>

<p><img src="/blog/images/2020-02-28-the-curious-case-of-kalman-filters-graph-representing-gaussian.png " /></p>

<p><strong><em>Gaussian is a continuous function over the space of location and the area underneath sums up to 1.</em></strong><br />
The Gaussian is characterized by two parameters, the mean, often abbreviated with the Greek letter Mu<strong><em>( μ )</em></strong>, and the width of the Gaussian often called the variance i.e. Sigma square<strong><em>( σ² )</em></strong>. So, our job in common phases is to maintain a Mu(μ) and a Sigma square<strong><em>( σ² )</em></strong> as our best estimate of the location of the object we are trying to find. Also, remember that the larger the width, the more uncertainty it possesses.</p>

<p><img src="/blog/images/2020-02-28-the-curious-case-of-kalman-filters-the-equation-for-1d-gaussian.png " /></p>

<p>The diagram in the above image represents the mean<strong><em>( μ )</em></strong> and variance<strong><em>( σ² )</em></strong> of the gaussian. The taller the mean<strong><em>( μ )</em></strong>, the chances of the object present at that position are higher. conversely, if the variance<strong><em>( σ² )</em></strong> is larger, i.e. wider the distribution, higher the uncertainty of that object; that might be positioned at any place within the gaussian. And as far as the formula is concerned, it is an exponential of a quadratic function where we take the exponent of the expression. The quadratic difference of our query point x, relative to the mean<strong><em>( μ )</em></strong>, divided by sigma square<strong><em>( σ² )</em></strong>, multiplied by -(1/2). Now if <strong><em>x</em></strong> = <strong><em>μ</em></strong>, then the numerator becomes 0, and if x of 0, which is 1. It turns out we have to normalize this by a constant, 1 over the square root of 2 Pi<strong><em>(π)</em></strong> sigma square<strong><em>( σ² )</em></strong>.</p>

<h1 id="gaussian-characteristics">Gaussian Characteristics:</h1>

<p>Gaussians are exponential function characterized by a given mean<strong><em>( μ )</em></strong>, which defines the location of the peak of a Gaussian curve, and a variance<strong><em>( σ² )</em></strong> which defines the width/spread of the curve. All Gaussian are:</p>
<ul>
  <li><strong>symmetrical</strong></li>
  <li>they have <strong>one peak</strong>, which is also referred to as a “unimodal” distribution, and they have an exponential drop off on either side of that peak.</li>
</ul>

<h1 id="more-on-variance">More on Variance:</h1>

<p>The variance is a measure of Gaussian spread; larger variances correspond to shorter Gaussians. Variance is also a measure of certainty; if you are trying to find something like the location of a car with the most certainty, you’ll want a Gaussian whose mean is the location of the car and with the smallest uncertainty/spread.</p>

<h1 id="lets-write-a-gaussian-function">Let’s write a Gaussian function:</h1>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># gaussian function
</span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="s">''' f takes in a mean and squared variance, and an input x
       and returns the gaussian value.'''</span>
    <span class="n">coefficient</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">pi</span> <span class="o">*</span><span class="n">sigma2</span><span class="p">)</span>
    <span class="n">exponential</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">sigma2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">coefficient</span> <span class="o">*</span> <span class="n">exponential</span>
</code></pre></div></div>
<h1 id="shifting-the-mean">Shifting the Mean:</h1>

<p>In Kalman filters, we iterate through measurement (measurement update) which uses Bayes rule, which is nothing else but a product or multiplication and through motion update(prediction) in which we use total probability which is a convolution or simply an addition.</p>

<p><img src="/blog/images/2020-02-28-the-curious-case-of-kalman-filters-mysterious-cycle.png " /></p>

<p>In order to understand the cycle, let’s assume that we are localizing a vehicle and we have a prior distribution(blue gaussian); it is a very wide Gaussian with the mean. and now let’s say we get a measurement(orange gaussian) that tells us something about the localization of vehicle. This is an example in our prior we were fairly uncertain about the location but the measurement told us quite a bit as to where the vehicle is.</p>

<p><img src="/blog/images/2020-02-28-the-curious-case-of-kalman-filters-prior-distribution-measurement-distribution.png " /></p>

<blockquote>
  <p>Note: In the above diagram, Mu<strong><em>( μ )</em></strong> is the prior mean and Nu<strong><em>( v )</em></strong> is the new measurement mean.</p>
</blockquote>

<p>The final mean gets shifted which is in between the two old means, the mean of the prior, and the mean of the measurement. It’s slightly further on the measurement side because the measurement was more certain as to where the vehicle is than prior. The more certain we are, the more we pull the mean on the direction of the certain answer.</p>

<p><img src="/blog/images/2020-02-28-the-curious-case-of-kalman-filters-new-mean.png " /></p>

<h1 id="where-is-the-new-peak">Where is the new peak?</h1>

<p><img src="/blog/images/2020-02-28-the-curious-case-of-kalman-filters-new-peak.png " /></p>

<p>The resulting Gaussian is more certain than the two-component Gaussians i.e. the covariance is smaller than either of the two covariances in the installation. Intuitively speaking, this is the case because we actually gain information. The two Gaussians together with high information content in either Gaussian installation.</p>

<h1 id="parameter-update">Parameter Update:</h1>

<p><img src="/blog/images/2020-02-28-the-curious-case-of-kalman-filters-measurement-update.png " /></p>

<ol>
  <li>Suppose we multiply two Gaussians, as in Bayes rule, a prior and a measurement probability. The prior has a mean of Mu<strong><em>( μ )</em></strong> and a variance of Sigma square<strong><em>( σ² )</em></strong>, and the measurement has a mean of Nu<strong><em>( v )</em></strong> and covariance of r-square<strong><em>( r² )</em></strong>.</li>
  <li>Then, the new mean, Mu prime<strong><em>( μ′ )</em></strong>, is the weighted sum of the old means. The Mu<strong><em>( μ )</em></strong> is weighted by r-square<strong><em>( r² )</em></strong>, Nu<strong><em>( v )</em></strong> is weighted by Sigma square<strong><em>( σ² )</em></strong>, normalized by the sum of the weighting factors. The new variance term would be Sigma square prime<strong><em>( σ²′ )</em></strong>.</li>
  <li>Clearly, the prior Gaussian has a much higher uncertainty, therefore, Sigma square<strong><em>( σ² )</em></strong> is larger and that means the Nu<strong><em>( v )</em></strong> is weighted much much larger than the Mu<strong><em>( μ )</em></strong>. So, the mean will be closer to the Nu<strong><em>( v )</em></strong>than the Mu<strong><em>( μ )</em></strong>. Interestingly enough, the variance term is unaffected by the actual means, it just uses the previous variances.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">mean1</span><span class="p">,</span> <span class="n">var1</span><span class="p">,</span> <span class="n">mean2</span><span class="p">,</span> <span class="n">var2</span><span class="p">):</span>
    <span class="s">''' This function takes in two means and two squared variance terms,
        and returns updated gaussian parameters.'''</span>
    <span class="c1"># Calculate the new parameters
</span>    <span class="n">new_mean</span> <span class="o">=</span> <span class="p">(</span><span class="n">var2</span><span class="o">*</span><span class="n">mean1</span> <span class="o">+</span> <span class="n">var1</span><span class="o">*</span><span class="n">mean2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">var2</span><span class="o">+</span><span class="n">var1</span><span class="p">)</span>
    <span class="n">new_var</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">var2</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="n">var1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">[</span><span class="n">new_mean</span><span class="p">,</span> <span class="n">new_var</span><span class="p">]</span>
</code></pre></div></div>

<h1 id="gaussian-motion">Gaussian Motion:</h1>

<p><img src="/blog/images/2020-02-28-the-curious-case-of-kalman-filters-motion-update.png " /></p>

<p>A new Mean<strong><em>( μ′ )</em></strong> is your old Mean Mu<strong><em>( μ )</em></strong> plus the motion often called <strong>u</strong>. So, if you move over 10 meters in the x-direction, this will be 10 meters and you knew Sigma square prime<strong><em>( σ²′ )</em></strong> is your old Sigma squared<strong><em>( σ² )</em></strong> plus the variance<strong><em>( r² )</em></strong> of the motion Gaussian. This is all you need to know, it’s just an addition. The resulting Gaussian in the prediction step just adds these two things up, mu<strong><em>( μ )</em></strong> plus <strong>u</strong> and sigma squared<strong><em>( σ² )</em></strong> plus <strong><em>( r² )</em></strong>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">mean1</span><span class="p">,</span> <span class="n">var1</span><span class="p">,</span> <span class="n">mean2</span><span class="p">,</span> <span class="n">var2</span><span class="p">):</span>
    <span class="s">''' This function takes in two means and two squared variance terms,
        and returns updated gaussian parameters, after motion.'''</span>
    <span class="c1"># Calculate the new parameters
</span>    <span class="n">new_mean</span> <span class="o">=</span> <span class="n">mean1</span> <span class="o">+</span> <span class="n">mean2</span>
    <span class="n">new_var</span> <span class="o">=</span> <span class="n">var1</span> <span class="o">+</span> <span class="n">var2</span>
    
    <span class="k">return</span> <span class="p">[</span><span class="n">new_mean</span><span class="p">,</span> <span class="n">new_var</span><span class="p">]</span>
</code></pre></div></div>

<h1 id="the-filter-pipeline">The Filter Pipeline:</h1>

<ul>
  <li>So now let’s put everything together. Let’s write the main program that takes these 2 functions, update and predict, and feeds into a sequence of measurements and motions. In the example I’ve chosen, measurements = 5., 6., 7., 9., 10. and motions are 1., 1., 2., 1., 1. This all would work out really well if the initial estimate was 5, but we’re setting it to 0 with a very large uncertainty of 10,000.</li>
  <li>Let’s assume the measurement uncertainty is constant 4, and the motion uncertainty is constant 2. When you run this, your first estimate for the position should basically become 5–4.99, and the reason is your initial uncertainty is so large, the estimate is dominated by the first measurement. Your uncertainty shrinks to 3.99, which is slightly better than the measurement uncertainty. You then predict that you add 1, but the uncertainty increases to 5.99, which is the motion uncertainty of 2.</li>
  <li>You update again based on measurement 6, you get your estimate of 5.99, which is almost 6. You move 1 again. You measure 7. You move 2. You measure 9. You move 1. You measure 10, and you move a final 1. And outcomes as the final result, a prediction of 10.99 for the position, which is your 10 position moved by 1, and the uncertainty(residual uncertainty) of 4.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># measurements for mu and motions, U
</span><span class="n">measurements</span> <span class="o">=</span> <span class="p">[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">]</span>
<span class="n">motions</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]</span>

<span class="c1"># initial parameters
</span><span class="n">measurement_sig</span> <span class="o">=</span> <span class="mf">4.</span>
<span class="n">motion_sig</span> <span class="o">=</span> <span class="mf">2.</span>
<span class="n">mu</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">sig</span> <span class="o">=</span> <span class="mf">10000.</span>  <span class="c1">#0000000001  
</span>
<span class="c1">## TODO: Loop through all measurements/motions
## Print out and display the resulting Gaussian 
</span>
<span class="c1"># your code here
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">measurements</span><span class="p">)):</span>
    <span class="c1"># measurement update, with uncertainty
</span>    <span class="n">mu</span><span class="p">,</span> <span class="n">sig</span> <span class="o">=</span> <span class="n">update</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sig</span><span class="p">,</span> <span class="n">measurements</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">measurement_sig</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Update: [{}, {}]'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sig</span><span class="p">))</span>
    <span class="c1"># motion update, with uncertainty
</span>    <span class="n">mu</span><span class="p">,</span> <span class="n">sig</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sig</span><span class="p">,</span> <span class="n">motions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">motion_sig</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Predict: [{}, {}]'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sig</span><span class="p">))</span>

    
<span class="c1"># print the final, resultant mu, sig
</span><span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Final result: [{}, {}]'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sig</span><span class="p">))</span>
</code></pre></div></div>
<h1 id="output">Output</h1>
<h4 id="when-the-initial-uncertainty-is-high">When the initial uncertainty is high:</h4>

<p><img src="/blog/images/2020-02-28-the-curious-case-of-kalman-filters-when-the-initial-uncertainty-is-high.png " /></p>

<h1 id="plotting-the-gaussian">Plotting the Gaussian:</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Print out and display the final, resulting Gaussian 
# set the parameters equal to the output of the Kalman filter result
</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
<span class="n">sigma2</span> <span class="o">=</span> <span class="n">sig</span>

<span class="c1"># define a range of x values
</span><span class="n">x_axis</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># create a corresponding list of gaussian values
</span><span class="n">g</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x_axis</span><span class="p">:</span>
    <span class="n">g</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>

<span class="c1"># plot the result 
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/blog/images/2020-02-28-the-curious-case-of-kalman-filters-resulting-gaussian.png " /></p>

<h1 id="how-the-filter-actually-works">How the Filter actually works:</h1>
<blockquote>
  <p>Remember that measurement_sig(var2 or σ²) = 4 in measurement update &amp; motion_sig(var2 or σ²) = 2 in motion update(prediction) will remain constant. Other than that each and every variable in the code will be updated at each timestep. for example, in the for loop of the filter pipeline, the mu and sig values will get updated in the measurement update and the new updated mu and sig values are then fed into the motion update(predict) function. then again when the new values are generated by the motion update(predict) function, those updated values are then fed into the measurement update(update) function and the cycle goes on till it calculates uncertainty for each and every timestep of the vehicle/object.
Feel free to mess around with the code and understand what is really happening when you change a particular variable’s value. <strong>Also, change the initial uncertainty to a very low value such as 0.0000000001 in order to get a clear understanding of How the Kalman Filter works!</strong></p>
</blockquote>

<p>Check out the code on <strong><a href="https://github.com/ksheersaagr/Kalman_Filters">Github</a></strong>.</p>]]></content><author><name>krunal kshirsagar</name></author><summary type="html"><![CDATA[Kalman filter finds the most optimum averaging factor for each consequent state. Also somehow remembers a little bit about the past states. It performs a joint probability distribution over the variables for each timeframe. The algorithm uses the new mean and new variance for every step in order to calculate the uncertainty and tries to provide accurate measurements for each timeframe of the measurement update (sensing/prediction) and the motion update(moving). The algorithm also uses other inaccuracies and statistical noise in order to represent the initial uncertainty.]]></summary></entry><entry><title type="html">Automatic Image Captioning with CNN &amp;amp; RNN</title><link href="https://ksheersaagr.github.io/blog/2020/01/automatic-image-captioning-with-cnn-rnn/" rel="alternate" type="text/html" title="Automatic Image Captioning with CNN &amp;amp; RNN" /><published>2020-01-20T00:00:00+00:00</published><updated>2020-01-20T00:00:00+00:00</updated><id>https://ksheersaagr.github.io/blog/2020/01/automatic-image-captioning-with-cnn-rnn</id><content type="html" xml:base="https://ksheersaagr.github.io/blog/2020/01/automatic-image-captioning-with-cnn-rnn/"><![CDATA[<p>Generally, a captioning model is a combination of two separate architecture that is CNN (Convolutional Neural Networks)&amp; RNN (Recurrent Neural Networks) and in this case LSTM (Long Short Term Memory), which is a special kind of RNN that includes a memory cell, in order to maintain the information for a longer period of time. Basically, CNN is used to generate feature vectors from the spatial data in the images and the vectors are fed through the fully connected linear layer into the RNN architecture in order to generate the sequential data or sequence of words that in the end generate description of an image by applying various image processing techniques to find the patterns in an image.</p>

<h1 id="dataset-used-for-training-the-model">Dataset used for Training the Model</h1>
<p>We’ll be using <a href="https://cocodataset.org/#download"><strong>MS-COCO dataset also stands for Microsoft Common Objects in COntext</strong></a>. This is an advance dataset where each image is paired with five associated captions that describes the content of that particular image. For example, If you were asked to write a caption that describes the image below, how would you do that?</p>

<p><img src="/blog/images/2020-01-20-automatic-image-captioning-with-cnn-rnn-bunch-of-goblins.png" /></p>

<p><em>First, you might look at the image and take note of different objects like different people and kites and blue sky. Then based on how these objects are placed in an image and their relationship with each other, you might think that these people are flying kites. They’re in this big grassy area, so they may also be in a park. After, collecting these visual observations you could put together a phrase that describes the image as, <strong>“People flying kite in a park”</strong>. We use a combination of spatial observation and sequential text descriptions to write a caption, and that’s exactly how the model that uses CNN and RNN architectures rolls.</em></p>

<h2 id="visualize-the-dataset">Visualize the Dataset</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="s">'/opt/cocoapi/PythonAPI'</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">pycocotools.coco</span> <span class="kn">import</span> <span class="n">COCO</span>

<span class="c1"># initialize COCO API for instance annotations
</span><span class="n">dataDir</span> <span class="o">=</span> <span class="s">'/opt/cocoapi'</span>
<span class="n">dataType</span> <span class="o">=</span> <span class="s">'val2014'</span>
<span class="n">instances_annFile</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">dataDir</span><span class="p">,</span> <span class="s">'annotations/instances_{}.json'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">dataType</span><span class="p">))</span>
<span class="n">coco</span> <span class="o">=</span> <span class="n">COCO</span><span class="p">(</span><span class="n">instances_annFile</span><span class="p">)</span>

<span class="c1"># initialize COCO API for caption annotations
</span><span class="n">captions_annFile</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">dataDir</span><span class="p">,</span> <span class="s">'annotations/captions_{}.json'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">dataType</span><span class="p">))</span>
<span class="n">coco_caps</span> <span class="o">=</span> <span class="n">COCO</span><span class="p">(</span><span class="n">captions_annFile</span><span class="p">)</span>

<span class="c1"># get image ids 
</span><span class="n">ids</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">coco</span><span class="p">.</span><span class="n">anns</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">skimage.io</span> <span class="k">as</span> <span class="n">io</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># pick a random image and obtain the corresponding URL
</span><span class="n">ann_id</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
<span class="n">img_id</span> <span class="o">=</span> <span class="n">coco</span><span class="p">.</span><span class="n">anns</span><span class="p">[</span><span class="n">ann_id</span><span class="p">][</span><span class="s">'image_id'</span><span class="p">]</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">coco</span><span class="p">.</span><span class="n">loadImgs</span><span class="p">(</span><span class="n">img_id</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">url</span> <span class="o">=</span> <span class="n">img</span><span class="p">[</span><span class="s">'coco_url'</span><span class="p">]</span>

<span class="c1"># print URL and visualize corresponding image
</span><span class="k">print</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">I</span> <span class="o">=</span> <span class="n">io</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">I</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># load and display captions
</span><span class="n">annIds</span> <span class="o">=</span> <span class="n">coco_caps</span><span class="p">.</span><span class="n">getAnnIds</span><span class="p">(</span><span class="n">imgIds</span><span class="o">=</span><span class="n">img</span><span class="p">[</span><span class="s">'id'</span><span class="p">]);</span>
<span class="n">anns</span> <span class="o">=</span> <span class="n">coco_caps</span><span class="p">.</span><span class="n">loadAnns</span><span class="p">(</span><span class="n">annIds</span><span class="p">)</span>
<span class="n">coco_caps</span><span class="p">.</span><span class="n">showAnns</span><span class="p">(</span><span class="n">anns</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/images/2020-01-20-automatic-image-captioning-with-cnn-rnn-coco-dataset.png " /></p>

<h2 id="the-cnn-rnn-architecture">The CNN-RNN Architecture</h2>

<h1 id="encoder-decoder">Encoder-Decoder:</h1>

<p><img src="/blog/images/2020-01-20-automatic-image-captioning-with-cnn-rnn-encoder-decoder.png " /></p>

<p>End to End, we want our captioning model to take in an image as input and output a text description of that image. The input image will be processed by a CNN and will connect the output of the CNN to the input of the RNN which will allow us to generate descriptive texts.</p>

<h1 id="resnet-architecture">ResNet Architecture:</h1>

<p><img src="/blog/images/2020-01-20-automatic-image-captioning-with-cnn-rnn-resnet-architecture.png " /></p>

<p>So, in order to generate a description, we feed a particular image into a pre-trained CNN like ResNet architecture. At the end of this network is a softmax classifier that outputs a vector of class scores but we don’t want to classify an image, instead we want a set of features that represents the spatial content in the image. To get that kind of spatial content, <strong>we’re going to remove the final fully connected layer that classifies the image</strong> and look at it’s earlier layer that distills the spatial information in the image.</p>

<h1 id="encoder-cnn">Encoder-CNN:</h1>

<p>Now, we’re using the CNN as a feature extractor that compresses the huge amount of extraction contained in the original image into a smaller representation. This <strong>CNN is often called the encoder because it encodes the content of the image into a smaller feature vector.</strong> Then we can process this feature vector and use it as an initial input to the following RNN.</p>

<p><img src="/blog/images/2020-01-20-automatic-image-captioning-with-cnn-rnn-cnn-encoder.png " /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EncoderCNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderCNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="n">resnet</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">resnet</span><span class="p">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        
        <span class="n">modules</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">resnet</span><span class="p">.</span><span class="n">children</span><span class="p">())[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">resnet</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">modules</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">resnet</span><span class="p">.</span><span class="n">fc</span><span class="p">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">images</span><span class="p">):</span>
        <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">resnet</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">features</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">embed</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">features</span>
</code></pre></div></div>

<h1 id="decoder-rnn">Decoder-RNN:</h1>

<p>The job of the RNN is to decode the process vector and turn it into a sequence of words. Thus, this portion of the network is often called a decoder.</p>

<p><img src="/blog/images/2020-01-20-automatic-image-captioning-with-cnn-rnn-rnn-decoder.png " /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">embed_size</span><span class="p">,</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">batch_first</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">captions</span><span class="p">):</span>
        <span class="n">captions</span> <span class="o">=</span> <span class="n">captions</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">embedding_layer</span><span class="p">(</span><span class="n">captions</span><span class="p">)</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">features</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">embed</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">lstm_outputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">embed</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear</span><span class="p">(</span><span class="n">lstm_outputs</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div>

<h2 id="loading-annotationstokenizing-captions">Loading Annotations/Tokenizing Captions:</h2>

<p>The RNN component of the captioning network is trained on the captions in the COCO dataset. We’re aiming to train the RNN to predict the next word of a sentence based on previous words. But, how exactly can it train on string data? Neural nets do not do well with strings. They need a well defined numerical alpha to effectively perform back-propagation and learn to produce similar output. So, we have to transform the captions associated with the image into a list of tokenized words. This tokenization turns any string into a list of words.</p>

<p><img src="/blog/images/2020-01-20-automatic-image-captioning-with-cnn-rnn-token.png " /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample_caption</span> <span class="o">=</span> <span class="s">'A person doing a trick on a rail while riding a skateboard.'</span>

<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">sample_tokens</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="n">tokenize</span><span class="p">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">sample_caption</span><span class="p">).</span><span class="n">lower</span><span class="p">())</span>

<span class="n">sample_caption</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">start_word</span> <span class="o">=</span> <span class="n">data_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="n">start_word</span>
<span class="n">sample_caption</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">data_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">vocab</span><span class="p">(</span><span class="n">start_word</span><span class="p">))</span>
<span class="n">sample_caption</span><span class="p">.</span><span class="n">extend</span><span class="p">([</span><span class="n">data_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">vocab</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">sample_tokens</span><span class="p">])</span>
<span class="n">end_word</span> <span class="o">=</span> <span class="n">data_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="n">end_word</span>
<span class="n">sample_caption</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">data_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">vocab</span><span class="p">(</span><span class="n">end_word</span><span class="p">))</span>
<span class="n">sample_caption</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">sample_caption</span><span class="p">).</span><span class="nb">long</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">sample_caption</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="working-of-tokenization">Working of Tokenization:</h2>

<p>First, we iterate through all of the training captions and create a dictionary that maps all unique words to a numerical index. So, every word we come across will have a corresponding integer value that can find in this dictionary. The words in this dictionary are referred to as our vocabulary. The vocabulary typically also includes a few special tokens.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Preview the word2idx dictionary.
</span><span class="nb">dict</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">data_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="n">word2idx</span><span class="p">.</span><span class="n">items</span><span class="p">())[:</span><span class="mi">10</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/blog/images/2020-01-20-automatic-image-captioning-with-cnn-rnn-dataloader-dataset.png " /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Modify the minimum word count threshold.
</span><span class="n">vocab_threshold</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># Obtain the data loader.
</span><span class="n">data_loader</span> <span class="o">=</span> <span class="n">get_loader</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">,</span>
                         <span class="n">mode</span><span class="o">=</span><span class="s">'train'</span><span class="p">,</span>
                         <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                         <span class="n">vocab_threshold</span><span class="o">=</span><span class="n">vocab_threshold</span><span class="p">,</span>
                         <span class="n">vocab_from_file</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Print the total number of keys in the word2idx dictionary.
</span><span class="k">print</span><span class="p">(</span><span class="s">'Total number of tokens in vocabulary:'</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">vocab</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/blog/images/2020-01-20-automatic-image-captioning-with-cnn-rnn-total-no-of-tokens.png " /></p>

<h2 id="embedding-layer">Embedding Layer:</h2>

<p><strong>There’s one more step before these words get sent as input to an RNN and thats the embedding layer, which transforms each word in a caption into a vector of a desired consistent shape.</strong></p>

<h1 id="words-to-vectors">Words to Vectors:</h1>

<p><img src="/blog/images/2020-01-20-automatic-image-captioning-with-cnn-rnn-word-2-vec.png " /></p>

<p>At this point, we know that you cannot directly feed words into an LSTM and expect it to be able to train or produce the correct output. These words first must be turned into a numerical representation so that a network can use normal loss functions and optimizers to calculate how “close” a predicted word and ground truth word (from a known, training caption) are? So, we typically turn a sequence of words into a sequence of numerical values; a vector of numbers where each number maps to a specific word in our vocabulary.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="s">" accepts pre-processed image tensor (inputs) and returns predicted sentence (list of tensor ids of length max_len) "</span>
    <span class="n">output_sentence</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span>
        <span class="n">lstm_outputs</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">states</span><span class="p">)</span>
        <span class="n">lstm_outputs</span> <span class="o">=</span> <span class="n">lstm_outputs</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear</span><span class="p">(</span><span class="n">lstm_outputs</span><span class="p">)</span>
        <span class="n">last_pick</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">output_sentence</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">last_pick</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">embedding_layer</span><span class="p">(</span><span class="n">last_pick</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">output_sentence</span>
</code></pre></div></div>

<h2 id="training-the-rnn-decoder-model-with-suitable-parameters">Training the RNN-Decoder model with suitable parameters:</h2>

<p>The Decoder will be made of LSTM cells which is good for remembering the lengthy sequences of words. Each LSTM cell is expecting to see the same shape of the input vector at each time-step. The very first cell is connected to the output feature vector of the CNN encoder. The input to the RNN for all future time steps will be the individual words of the training caption. So, at the start of training, we have some input from our CNN, and LSTM cell with initial state. Now the RNN has two responsibilities:</p>
<ol>
  <li>To Remember spatial information from the input feature vector.</li>
  <li>To Predict the next word.</li>
</ol>

<p>We know that the very first word it produces should always be the <code class="highlighter-rouge">&lt;start&gt;</code> token and the next word should be those in the training caption. At every time step, we look at the current caption word as input and combine it with the hidden state of the LSTM cell to produce an output. This output is then passed to the fully connected layer that produces a distribution that represents the most likely next word. We feed the next word in the caption to the network and so on until we reach the <code class="highlighter-rouge">&lt;end&gt;</code>token. The hidden state of an LSTM is a function of the input token to the LSTM and the previous state also referred to as the recurrence function. The recurrence function is defined by weights and during the training process, this model uses back-propagation to update these weights until the LSTM cells learn to produce the correct next word in the caption given the current input word. As with most models, you can also take advantage of batching the training data. The model updates its weights after each training batch with the batch size is the number of image caption pairs sent through the network during a single training step. Once the model has trained, it will have learned from many image caption pairs and should be able to generate captions for new image data.</p>

<p><strong>Note: Please do play around with hyperparameters if you don’t get the desired result. I’ve nailed the hyperparameters by setting them to particular value based on instinct in one go. Also, please make sure not to change the values of mean &amp; standard deviation in transforms.Normalize() as those values are default and are considered after rigorous training of ResNet architecture on the ImageNet Dataset.</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="s">'/opt/cocoapi/PythonAPI'</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">pycocotools.coco</span> <span class="kn">import</span> <span class="n">COCO</span>
<span class="kn">from</span> <span class="nn">data_loader</span> <span class="kn">import</span> <span class="n">get_loader</span>
<span class="kn">from</span> <span class="nn">model</span> <span class="kn">import</span> <span class="n">EncoderCNN</span><span class="p">,</span> <span class="n">DecoderRNN</span>
<span class="kn">import</span> <span class="nn">math</span>


<span class="c1">## TODO #1: Select appropriate values for the Python variables below.
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>          <span class="c1"># batch size
</span><span class="n">vocab_threshold</span> <span class="o">=</span> <span class="mi">5</span>        <span class="c1"># minimum word count threshold
</span><span class="n">vocab_from_file</span> <span class="o">=</span> <span class="bp">True</span>    <span class="c1"># if True, load existing vocab file
</span><span class="n">embed_size</span> <span class="o">=</span> <span class="mi">300</span>           <span class="c1"># dimensionality of image and word embeddings
</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">512</span>          <span class="c1"># number of features in hidden state of the RNN decoder
</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">3</span>             <span class="c1"># number of training epochs
</span><span class="n">save_every</span> <span class="o">=</span> <span class="mi">1</span>             <span class="c1"># determines frequency of saving model weights
</span><span class="n">print_every</span> <span class="o">=</span> <span class="mi">100</span>          <span class="c1"># determines window for printing average loss
</span><span class="n">log_file</span> <span class="o">=</span> <span class="s">'training_log.txt'</span>       <span class="c1"># name of file with saved training loss and perplexity
</span>
<span class="c1"># (Optional) TODO #2: Amend the image transform below.
</span><span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span> 
    <span class="n">transforms</span><span class="p">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>                          <span class="c1"># smaller edge of image resized to 256
</span>    <span class="n">transforms</span><span class="p">.</span><span class="n">RandomCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>                      <span class="c1"># get 224x224 crop from random location
</span>    <span class="n">transforms</span><span class="p">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>               <span class="c1"># horizontally flip image with probability=0.5
</span>    <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>                           <span class="c1"># convert the PIL Image to a tensor
</span>    <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">),</span>      <span class="c1"># normalize image for pre-trained model
</span>                         <span class="p">(</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">))])</span>

<span class="c1"># Build data loader.
</span><span class="n">data_loader</span> <span class="o">=</span> <span class="n">get_loader</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">,</span>
                         <span class="n">mode</span><span class="o">=</span><span class="s">'train'</span><span class="p">,</span>
                         <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                         <span class="n">vocab_threshold</span><span class="o">=</span><span class="n">vocab_threshold</span><span class="p">,</span>
                         <span class="n">vocab_from_file</span><span class="o">=</span><span class="n">vocab_from_file</span><span class="p">)</span>

<span class="c1"># The size of the vocabulary.
</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">vocab</span><span class="p">)</span>

<span class="c1"># Initialize the encoder and decoder. 
</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">EncoderCNN</span><span class="p">(</span><span class="n">embed_size</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">DecoderRNN</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

<span class="c1"># Move models to GPU if CUDA is available. 
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>
<span class="n">encoder</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">decoder</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Define the loss function. 
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">().</span><span class="n">cuda</span><span class="p">()</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># TODO #3: Specify the learnable parameters of the model.
</span><span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">decoder</span><span class="p">.</span><span class="n">parameters</span><span class="p">())</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">encoder</span><span class="p">.</span><span class="n">embed</span><span class="p">.</span><span class="n">parameters</span><span class="p">())</span>

<span class="c1"># TODO #4: Define the optimizer.
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span> <span class="o">=</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># Set the total number of training steps per epoch.
</span><span class="n">total_step</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">caption_lengths</span><span class="p">)</span> <span class="o">/</span> <span class="n">data_loader</span><span class="p">.</span><span class="n">batch_sampler</span><span class="p">.</span><span class="n">batch_size</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Lowest Loss: 1.74 after more than 6 hours of training:</strong></p>

<p><img src="/blog/images/2020-01-20-automatic-image-captioning-with-cnn-rnn-lowest-loss-6-hours-of-training.png " /></p>

<p><strong><em>Do test the model on Test/Validation Data to check for overfitting as the above result is of the training set.</em></strong></p>

<h2 id="generate-predictions">Generate Predictions:</h2>

<p>A function <strong><code class="highlighter-rouge">(get_prediction)</code></strong> used to loop over images in the test dataset and print your model’s predicted caption.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_prediction</span><span class="p">():</span>
    <span class="n">orig_image</span><span class="p">,</span> <span class="n">image</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">orig_image</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Sample Image'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">features</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">image</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>    
    <span class="n">sentence</span> <span class="o">=</span> <span class="n">clean_sentence</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="output">Output:</h1>

<p>Call the <strong><code class="highlighter-rouge">(get_prediction)</code></strong> function every time you want the result.</p>

<h3 id="when-the-model-performed-better">When the model performed better:</h3>

<p><img src="/blog/images/2020-01-20-automatic-image-captioning-with-cnn-rnn-result-when-model-perform-better.png " /></p>

<p><img src="/blog/images/2020-01-20-automatic-image-captioning-with-cnn-rnn-result-when-model-perform-better-1.png " /></p>

<h3 id="when-the-model-didnt-perform-well">When the model didn’t perform well:</h3>

<p><img src="/blog/images/2020-01-20-automatic-image-captioning-with-cnn-rnn-result-when-model-not-perform-that-well.png " /></p>

<p><img src="/blog/images/2020-01-20-automatic-image-captioning-with-cnn-rnn-result-when-model-not-perform-that-well-1.png " /></p>

<p><strong>Clearly, as you can see the model struggles if the image is cluttered with more objects. Hence, the model finds it difficult to generate a long sequence of words that relate to each other using the spatial data in the image.</strong></p>

<p>Make sure to check out my project on <strong><a href="https://github.com/ksheersaagr/Automatic-Image-Captioning/">Github</a>.</strong></p>

<h2 id="references">References:</h2>

<ol>
  <li>
    <p><a href="https://arxiv.org/pdf/1502.03044.pdf">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></p>
  </li>
  <li>
    <p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Chris Olah’s blog</a></p>
  </li>
  <li>
    <p><a href="http://blog.echen.me/2017/05/30/exploring-lstms/">Exploring LSTMs</a></p>
  </li>
  <li>
    <p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Karpathy’s Blog on RNN</a></p>
  </li>
  <li>
    <p><a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture10.pdf">RNN Slides of CS231n Lecture 10 of 2019- Fei-Fei Li</a></p>
  </li>
  <li>
    <p><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf">Detection and Segmentation slides of CS231n Lecture 11 of 2017- Fei-Fei Li</a></p>
  </li>
  <li>
    <p><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks">RNN Cheatsheet</a></p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=6niqTuYFZLQ&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&amp;index=10">Convolutional Neural Networks for Visual Recognition Spring 2017 Stanford Youtube</a></p>
  </li>
</ol>]]></content><author><name>krunal kshirsagar</name></author><summary type="html"><![CDATA[Generally, a captioning model is a combination of two separate architecture that is CNN (Convolutional Neural Networks)&amp; RNN (Recurrent Neural Networks) and in this case LSTM (Long Short Term Memory), which is a special kind of RNN that includes a memory cell, in order to maintain the information for a longer period of time. Basically, CNN is used to generate feature vectors from the spatial data in the images and the vectors are fed through the fully connected linear layer into the RNN architecture in order to generate the sequential data or sequence of words that in the end generate description of an image by applying various image processing techniques to find the patterns in an image.]]></summary></entry><entry><title type="html">Facial Keypoint Detection using CNN &amp;amp; PyTorch</title><link href="https://ksheersaagr.github.io/blog/2019/12/facial-keypoint-detection-using-cnn-pytorch/" rel="alternate" type="text/html" title="Facial Keypoint Detection using CNN &amp;amp; PyTorch" /><published>2019-12-01T00:00:00+00:00</published><updated>2019-12-01T00:00:00+00:00</updated><id>https://ksheersaagr.github.io/blog/2019/12/facial-keypoint-detection-using-cnn-pytorch</id><content type="html" xml:base="https://ksheersaagr.github.io/blog/2019/12/facial-keypoint-detection-using-cnn-pytorch/"><![CDATA[<p>Facial Keypoints are also called Facial Landmarks which generally specify the areas of the nose, eyes, mouth, etc on the face, classified by <strong>68 key points, with coordinates (x, y), for that face</strong>. With Facial Keypoints, we can achieve facial recognition, emotion recognition, etc.</p>

<p><img src="/blog/images/2019-12-01-facial-keypoint-detection-using-cnn-pytorch-dots-represents-keypoints.png" /></p>

<h2 id="selecting-the-dataset">Selecting the Dataset:</h2>

<p>We’ll be using <a href="https://www.cs.tau.ac.il/~wolf/ytfaces/">YouTube Faces Dataset</a>. It is a dataset that contains 3,425 face videos designed for studying the problem of unconstrained face recognition in videos. These videos have been fed through processing steps and turned into sets of image frames containing one face and the associated keypoints.</p>

<h2 id="training-and-test-data">Training and Test Data:</h2>

<blockquote>
  <p><em>This facial keypoints dataset consists of 5770 color images. All of these images are separated into either a training or a test set of data. 3462 of these images are training images, for you to use as you create a model to predict keypoints. 2308 are test images, which will be used to test the accuracy of your model.</em></p>
</blockquote>

<h2 id="pre-processing-the-data">Pre-Processing the Data:</h2>

<p>In order to feed the data(images) into the neural network, we have to transform the images into a fixed dimensional size and a standard color range by converting the <a href="https://rickwierenga.com/blog/fast.ai/FastAI2019-8.html">numpy arrays to Pytorch Tensors</a>for faster computation.</p>

<h3 id="transforms">Transforms:</h3>
<ul>
  <li>
    <h4 id="normalize"><code class="highlighter-rouge">Normalize</code>:</h4>
    <p>to convert a color image to grayscale values with a range of [0, 1] and normalize the keypoints to be in a range of about [-1, 1].</p>
  </li>
  <li>
    <h4 id="rescale"><code class="highlighter-rouge">Rescale</code>:</h4>
    <p>to rescale an image to a desired size.</p>
  </li>
  <li>
    <h4 id="randomcrop"><code class="highlighter-rouge">RandomCrop</code>:</h4>
    <p>to crop an image randomly.</p>
  </li>
  <li>
    <h4 id="totensor"><code class="highlighter-rouge">ToTensor</code>:</h4>
    <p>to convert numpy images to torch images.</p>
  </li>
</ul>

<p>Using Transformation techniques:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># test out some of these transforms
</span><span class="n">rescale</span> <span class="o">=</span> <span class="n">Rescale</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">crop</span> <span class="o">=</span> <span class="n">RandomCrop</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">composed</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">Rescale</span><span class="p">(</span><span class="mi">250</span><span class="p">),</span>
                               <span class="n">RandomCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">)])</span>

<span class="c1"># apply the transforms to a sample image
</span><span class="n">test_num</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">face_dataset</span><span class="p">[</span><span class="n">test_num</span><span class="p">]</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">rescale</span><span class="p">,</span> <span class="n">crop</span><span class="p">,</span> <span class="n">composed</span><span class="p">]):</span>
    <span class="n">transformed_sample</span> <span class="o">=</span> <span class="n">tx</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">tx</span><span class="p">).</span><span class="n">__name__</span><span class="p">)</span>
    <span class="n">show_keypoints</span><span class="p">(</span><span class="n">transformed_sample</span><span class="p">[</span><span class="s">'image'</span><span class="p">],</span> <span class="n">transformed_sample</span><span class="p">[</span><span class="s">'keypoints'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Output of Transformation:</strong></p>

<p><img src="/blog/images/2019-12-01-facial-keypoint-detection-using-cnn-pytorch-output-of-transformation.png" /></p>

<p><strong>Creating the Transformed Dataset:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define the data tranform
# order matters! i.e. rescaling should come before a smaller crop
</span><span class="n">data_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">Rescale</span><span class="p">(</span><span class="mi">250</span><span class="p">),</span>
                                     <span class="n">RandomCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
                                     <span class="n">Normalize</span><span class="p">(),</span>
                                     <span class="n">ToTensor</span><span class="p">()])</span>

<span class="c1"># create the transformed dataset
</span><span class="n">transformed_dataset</span> <span class="o">=</span> <span class="n">FacialKeypointsDataset</span><span class="p">(</span><span class="n">csv_file</span><span class="o">=</span><span class="s">'/data/training_frames_keypoints.csv'</span><span class="p">,</span>
                                             <span class="n">root_dir</span><span class="o">=</span><span class="s">'/data/training/'</span><span class="p">,</span>
                                             <span class="n">transform</span><span class="o">=</span><span class="n">data_transform</span><span class="p">)</span>
</code></pre></div></div>

<p>Here <strong><em>224 * 224px</em></strong> are standardized input image size that is obtained by transforms and the output class scores shall be <strong><em>136</em></strong> i.e. <strong><em>136/2 = 68</em></strong></p>

<p><img src="/blog/images/2019-12-01-facial-keypoint-detection-using-cnn-pytorch-standard-input-size-of-images.png" /></p>

<h2 id="define-the-cnn-architecture">Define the CNN Architecture:</h2>

<p>After you’ve looked at the data you’re working with and, in this case, know the shapes of the images and of the keypoints, you are ready to define a convolutional neural network that can learn from this data.</p>

<h4 id="define-all-the-layers-of-this-cnn-the-only-requirements-are">Define all the layers of this CNN, the only requirements are:</h4>

<ol>
  <li>This network takes in a square (same width and height), grayscale image as input.</li>
  <li>It ends with a linear layer that represents the keypoints (Last layer output 136 values, 2 for each of the 68 keypoint (x, y) pairs).</li>
</ol>

<p><strong>Shape of a Convolutional Layer:</strong></p>
<blockquote>
  <ul>
    <li>K — out_channels : the number of filters in the convolutional layer</li>
    <li>F — kernel_size</li>
    <li>S — the stride of the convolution</li>
    <li>P — the padding</li>
    <li>W — the width/height (square) of the previous layer</li>
  </ul>
</blockquote>

<p>The <strong>self.conv1</strong> = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)</p>

<p><strong>output size = (W-F)/S +1</strong> = (224–5)/1 +1 = 220, the output Tensor for one image will have the dimensions: (1, 220, 220)</p>

<p><strong>1</strong> = input image channel (grayscale), <strong>32</strong> = output channels/feature maps, <strong>5x5</strong> = square convolution kernel</p>

<h3 id="cnn-architecture">CNN Architecture:</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="c1"># output size = (W-F)/S +1 = (224-5)/1 + 1 = 220
</span><span class="bp">self</span><span class="p">.</span><span class="n">pool1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># 220/2 = 110  the output Tensor for one image, will have the dimensions: (32, 110, 110)
</span>
<span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># output size = (W-F)/S +1 = (110-3)/1 + 1 = 108
</span><span class="bp">self</span><span class="p">.</span><span class="n">pool2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1">#108/2=54   the output Tensor for one image, will have the dimensions: (64, 54, 54)
</span>
<span class="bp">self</span><span class="p">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># output size = (W-F)/S +1 = (54-3)/1 + 1 = 52
</span><span class="bp">self</span><span class="p">.</span><span class="n">pool3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1">#52/2=26    the output Tensor for one image, will have the dimensions: (128, 26, 26)
</span>
<span class="bp">self</span><span class="p">.</span><span class="n">conv4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># output size = (W-F)/S +1 = (26-3)/1 + 1 = 24
</span><span class="bp">self</span><span class="p">.</span><span class="n">pool4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1">#24/2=12   the output Tensor for one image, will have the dimensions: (256, 12, 12)
</span>
<span class="bp">self</span><span class="p">.</span><span class="n">conv5</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># output size = (W-F)/S +1 = (12-1)/1 + 1 = 12
</span><span class="bp">self</span><span class="p">.</span><span class="n">pool5</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1">#12/2=6    the output Tensor for one image, will have the dimensions: (512, 6, 6)
</span>
<span class="c1">#Linear Layer
</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="o">*</span><span class="mi">6</span><span class="o">*</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">136</span><span class="p">)</span>
</code></pre></div></div>
<p>We can add <a href="https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/"><code class="highlighter-rouge">Dropouts</code></a> for Regularizing Deep Neural Networks. One of the secrets to achieving better results is to keep the probability(p) of dropouts within the range of 0.1 to 0.5. also, it’s better to have multiple dropouts of varying values of probability(p).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">drop1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">drop2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">drop3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">drop4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">drop5</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">drop6</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="mf">0.4</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li><strong>Next, We’ll construct our Feed-Forward Network having <a href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html"><code class="highlighter-rouge">ReLU</code></a> as our activation function.</strong></li>
</ul>

<h4 id="feedforward-neural-network">Feedforward Neural Network:</h4>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1">## TODO: Define the feedforward behavior of this model</span>
        <span class="c1">## x is the input image and, as an example, here you may choose to include a pool/conv step:</span>
        <span class="c1">## x = self.pool(F.relu(self.conv1(x)))</span>
      
        <span class="n">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nf">pool1</span><span class="p">(</span><span class="no">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nf">drop1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nf">pool2</span><span class="p">(</span><span class="no">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nf">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nf">drop2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nf">pool3</span><span class="p">(</span><span class="no">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nf">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nf">drop3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nf">pool4</span><span class="p">(</span><span class="no">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nf">conv4</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nf">drop4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nf">pool5</span><span class="p">(</span><span class="no">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nf">conv5</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nf">drop5</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="no">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nf">drop6</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nf">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># a modified x, having gone through all the layers of your model, should be returned</span>
        <span class="k">return</span> <span class="n">x</span></code></pre></figure>

<ul>
  <li><strong>Create the transformed Facial Keypoints Dataset, just as before</strong></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create the transformed dataset
</span><span class="n">transformed_dataset</span> <span class="o">=</span> <span class="n">FacialKeypointsDataset</span><span class="p">(</span><span class="n">csv_file</span><span class="o">=</span><span class="s">'/data/training_frames_keypoints.csv'</span><span class="p">,</span>
                                             <span class="n">root_dir</span><span class="o">=</span><span class="s">'/data/training/'</span><span class="p">,</span>
                                             <span class="n">transform</span><span class="o">=</span><span class="n">data_transform</span><span class="p">)</span>


<span class="k">print</span><span class="p">(</span><span class="s">'Number of images: '</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformed_dataset</span><span class="p">))</span>

<span class="c1"># iterate through the transformed dataset and print some stats about the first few samples
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">transformed_dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">sample</span><span class="p">[</span><span class="s">'image'</span><span class="p">].</span><span class="n">size</span><span class="p">(),</span> <span class="n">sample</span><span class="p">[</span><span class="s">'keypoints'</span><span class="p">].</span><span class="n">size</span><span class="p">())</span>
</code></pre></div></div>

<ul>
  <li><strong>Batching and loading data</strong><br />
<em>Next, having defined the transformed dataset, we can use PyTorch’s DataLoader class to load the training data in batches of whatever size as well as to shuffle the data for training the model.</em></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load training data in batches
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">transformed_dataset</span><span class="p">,</span> 
                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                          <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                          <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><strong>Train the CNN Model and Track the loss</strong></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## TODO: Define the loss and optimization
</span><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">SmoothL1Loss</span><span class="p">()</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span>
</code></pre></div></div>
<p><strong><em>Note: Please try using a different criterion <a href="https://pytorch.org/docs/master/_modules/torch/nn/modules/loss.html"><code class="highlighter-rouge">Loss function</code></a> and also have the value of the learning rate set to the lowest number possible; in this case(0.001).</em></strong></p>

<ul>
  <li>
    <p><strong>Training and Initial Observation</strong><br />
To quickly observe how our model is training and decide on whether or not we should modify its structure or hyperparameters, you’re encouraged to start off with just one or two epochs at first. As you train, note how your model’s loss behaves over time: does it decrease quickly at first and then slow down? Does it take a while to decrease in the first place? What happens if we change the batch size of your training data or modify your loss function? etc.<br />
Use these initial observations to make changes to your model and decide on the best architecture before you train for many epochs and create a final model.</p>
  </li>
  <li>
    <p><strong>Training Loss:</strong><br />
<img src="/blog/images/2019-12-01-facial-keypoint-detection-using-cnn-pytorch-training-loss.png" /><br />
Once you’ve found a good model, save it. So that you can load and use it later.<br />
After you’ve trained a neural network to detect facial keypoints, you can then apply this network to any image that includes faces.</p>
  </li>
  <li>
    <p><strong>Detect faces in any image using Haar Cascade Detector in the project</strong></p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load in a haar cascade classifier for detecting frontal faces
</span><span class="n">face_cascade</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">CascadeClassifier</span><span class="p">(</span><span class="s">'detector_architectures/haarcascade_frontalface_default.xml'</span><span class="p">)</span>

<span class="c1"># run the detector
# the output here is an array of detections; the corners of each detection box
# if necessary, modify these parameters until you successfully identify every face in a given image
</span><span class="n">faces</span> <span class="o">=</span> <span class="n">face_cascade</span><span class="p">.</span><span class="n">detectMultiScale</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># make a copy of the original image to plot detections on
</span><span class="n">image_with_detections</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># loop over the detected faces, mark the image where each face is found
</span><span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">h</span><span class="p">)</span> <span class="ow">in</span> <span class="n">faces</span><span class="p">:</span>
    <span class="c1"># draw a rectangle around each detected face
</span>    <span class="c1"># you may also need to change the width of the rectangle drawn depending on image resolution
</span>    <span class="n">cv2</span><span class="p">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">image_with_detections</span><span class="p">,(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),(</span><span class="n">x</span><span class="o">+</span><span class="n">w</span><span class="p">,</span><span class="n">y</span><span class="o">+</span><span class="n">h</span><span class="p">),(</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">3</span><span class="p">)</span> 

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_with_detections</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li><strong>Haar Cascade Detector</strong><br />
<img src="/blog/images/2019-12-01-facial-keypoint-detection-using-cnn-pytorch-haar-cascade-detector.png" />
    <h3 id="transform-each-detected-face-into-an-input-tensor">Transform each detected face into an input Tensor</h3>
    <p>You’ll need to perform the following steps for each detected face:</p>
    <ol>
      <li>Convert the face from RGB to grayscale</li>
      <li>Normalize the grayscale image so that its color range falls in [0,1] instead of [0,255]</li>
      <li>Rescale the detected face to be the expected square size for your CNN (224x224, suggested)</li>
      <li>Reshape the numpy image into a torch image <br />
<img src="/blog/images/2019-12-01-facial-keypoint-detection-using-cnn-pytorch-mitchelle-keypoints.png" /></li>
    </ol>
  </li>
</ul>

<h3 id="detect-and-display-the-predicted-keypoints">Detect and display the predicted keypoints</h3>
<p>After each face has been appropriately converted into an input Tensor for our network to see as input, we can apply the network to each face. The output should be the predicted facial keypoints.<br />
These keypoints will need to be “un-normalized” for display, and you may find it helpful to write a helper function like <code class="highlighter-rouge">show_keypoints</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">showpoints</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="n">keypoints</span><span class="p">):</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
    
    <span class="n">keypoints</span> <span class="o">=</span> <span class="n">keypoints</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">keypoints</span> <span class="o">=</span> <span class="n">keypoints</span> <span class="o">*</span> <span class="mf">60.0</span> <span class="o">+</span> <span class="mi">68</span>
    <span class="n">keypoints</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">keypoints</span><span class="p">,</span> <span class="p">(</span><span class="mi">68</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">keypoints</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">keypoints</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'.'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">)</span>
    

<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="n">image_copy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

<span class="c1"># loop over the detected faces from your haar cascade
</span><span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">h</span><span class="p">)</span> <span class="ow">in</span> <span class="n">faces</span><span class="p">:</span>
    
    <span class="c1"># Select the region of interest that is the face in the image 
</span>    <span class="n">roi</span> <span class="o">=</span> <span class="n">image_copy</span><span class="p">[</span><span class="n">y</span><span class="p">:</span><span class="n">y</span><span class="o">+</span><span class="n">h</span><span class="p">,</span><span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="o">+</span><span class="n">w</span><span class="p">]</span>

    <span class="c1">## TODO: Convert the face region from RGB to grayscale
</span>    <span class="n">roi</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">roi</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_RGB2GRAY</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">roi</span>

    <span class="c1">## TODO: Normalize the grayscale image so that its color range falls in [0,1] instead of [0,255]
</span>    <span class="n">roi</span> <span class="o">=</span> <span class="n">roi</span><span class="o">/</span><span class="mf">255.0</span>
    
    <span class="c1">## TODO: Rescale the detected face to be the expected square size for your CNN (224x224, suggested)
</span>    <span class="n">roi</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="n">roi</span><span class="p">,</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">))</span>
    
    <span class="c1">## TODO: Reshape the numpy image shape (H x W x C) into a torch image shape (C x H x W)
</span>    <span class="n">roi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">roi</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">roi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">roi</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    
    <span class="c1">## TODO: Make facial keypoint predictions using your loaded, trained network 
</span>    <span class="n">roi_torch</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">roi</span><span class="p">))</span>
    
    <span class="n">roi_torch</span> <span class="o">=</span> <span class="n">roi_torch</span><span class="p">.</span><span class="nb">type</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">)</span>
    <span class="n">keypoints</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">roi_torch</span><span class="p">)</span>

    <span class="c1">## TODO: Display each detected face and the corresponding keypoints        
</span>    <span class="n">showpoints</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="n">keypoints</span><span class="p">)</span>
</code></pre></div></div>
<h4 id="output">Output:</h4>
<p><img src="/blog/images/2019-12-01-facial-keypoint-detection-using-cnn-pytorch-facial-keypoints-detected.png" /></p>

<p><strong>Feel free to check out my project on <a href="https://github.com/ksheersaagr/Facial_Keypoint_Detection">Github</a>.</strong></p>]]></content><author><name>krunal kshirsagar</name></author><summary type="html"><![CDATA[Facial Keypoints are also called Facial Landmarks which generally specify the areas of the nose, eyes, mouth, etc on the face, classified by 68 key points, with coordinates (x, y), for that face. With Facial Keypoints, we can achieve facial recognition, emotion recognition, etc.]]></summary></entry><entry><title type="html">How to build a Reddit bot to scrape saved posts by filtering out NSFW posts</title><link href="https://ksheersaagr.github.io/blog/2019/10/how-to-build-a-reddit-bot-to-scrape-saved-posts-by-filtering-out-nsfw-posts/" rel="alternate" type="text/html" title="How to build a Reddit bot to scrape saved posts by filtering out NSFW posts" /><published>2019-10-23T00:00:00+00:00</published><updated>2019-10-23T00:00:00+00:00</updated><id>https://ksheersaagr.github.io/blog/2019/10/how-to-build-a-reddit-bot-to-scrape-saved-posts-by-filtering-out-nsfw-posts</id><content type="html" xml:base="https://ksheersaagr.github.io/blog/2019/10/how-to-build-a-reddit-bot-to-scrape-saved-posts-by-filtering-out-nsfw-posts/"><![CDATA[<p>This script filters out the NSFW posts and scrapes only the Non-NSFW posts which are stored in <code class="highlighter-rouge">saved posts</code> of your Reddit account into a <code class="highlighter-rouge">.csv</code> file.<br />
As I was implementing this script, I realized that my code isn’t universal. Therefore, I would like to provide you with my code and in order to get the script running for you, you need to tweak some class names or element id according to your Reddit account via CSS query selector by inspecting elements in your browser.</p>

<p>However, Do run the script initially by inserting your credentials wherever asked. for example wherever there is <code class="highlighter-rouge">Your-username</code> or <code class="highlighter-rouge">Your-password</code> written in the script, do insert your Reddit <strong><em>username/email id</em></strong> and <strong><em>password</em></strong> there.</p>

<h3 id="heres-how-to-do-it">Here’s how to do it:</h3>
<ol>
  <li>You should have selenium web driver installed in your python environment, To install selenium enter below command in the command prompt or anaconda prompt or vs code terminal:<br />
<strong><code class="highlighter-rouge">pip install selenium</code></strong></li>
  <li>After installing selenium, make sure you have a firefox browser installed on your system.</li>
  <li>You need some knowledge of CSS query selector in order to access the elements via class name on Reddit by inspecting element on the browser(firefox).</li>
  <li>The code isn’t universal so you need to tweak the code according to your Reddit Classnames by inspecting elements and using CSS query selectors on the browser.</li>
</ol>

<p>Checkout the code on <strong><a href="https://github.com/ksheersaagr/reddit_saved_post_filter">Github</a></strong></p>]]></content><author><name>krunal kshirsagar</name></author><summary type="html"><![CDATA[This script filters out the NSFW posts and scrapes only the Non-NSFW posts which are stored in saved posts of your Reddit account into a .csv file. As I was implementing this script, I realized that my code isn’t universal. Therefore, I would like to provide you with my code and in order to get the script running for you, you need to tweak some class names or element id according to your Reddit account via CSS query selector by inspecting elements in your browser.]]></summary></entry></feed>